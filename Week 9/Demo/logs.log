2024-04-22 10:48:05,629:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:48:05,629:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:48:05,629:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:48:05,629:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:48:11,901:INFO:PyCaret ClassificationExperiment
2024-04-22 10:48:11,901:INFO:Logging name: clf-default-name
2024-04-22 10:48:11,902:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-04-22 10:48:11,902:INFO:version 3.2.0
2024-04-22 10:48:11,902:INFO:Initializing setup()
2024-04-22 10:48:11,902:INFO:self.USI: 779d
2024-04-22 10:48:11,903:INFO:self._variable_keys: {'gpu_n_jobs_param', 'idx', '_ml_usecase', 'USI', 'target_param', 'seed', 'exp_id', 'fold_generator', 'exp_name_log', 'n_jobs_param', 'fold_groups_param', 'X_test', 'y_test', 'X_train', 'fix_imbalance', 'pipeline', 'y_train', 'logging_param', 'X', 'y', 'data', '_available_plots', 'memory', 'html_param', 'gpu_param', 'fold_shuffle_param', 'is_multiclass', 'log_plots_param'}
2024-04-22 10:48:11,903:INFO:Checking environment
2024-04-22 10:48:11,903:INFO:python_version: 3.11.0
2024-04-22 10:48:11,903:INFO:python_build: ('main', 'Oct 24 2022 18:26:48')
2024-04-22 10:48:11,903:INFO:machine: AMD64
2024-04-22 10:48:11,903:INFO:platform: Windows-10-10.0.22631-SP0
2024-04-22 10:48:11,910:INFO:Memory: svmem(total=3946209280, available=246063104, percent=93.8, used=3700146176, free=246063104)
2024-04-22 10:48:11,911:INFO:Physical Core: 2
2024-04-22 10:48:11,911:INFO:Logical Core: 4
2024-04-22 10:48:11,911:INFO:Checking libraries
2024-04-22 10:48:11,911:INFO:System:
2024-04-22 10:48:11,911:INFO:    python: 3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]
2024-04-22 10:48:11,911:INFO:executable: c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\python.exe
2024-04-22 10:48:11,911:INFO:   machine: Windows-10-10.0.22631-SP0
2024-04-22 10:48:11,911:INFO:PyCaret required dependencies:
2024-04-22 10:48:14,189:INFO:                 pip: 24.0
2024-04-22 10:48:14,189:INFO:          setuptools: 68.2.2
2024-04-22 10:48:14,189:INFO:             pycaret: 3.2.0
2024-04-22 10:48:14,189:INFO:             IPython: 8.16.1
2024-04-22 10:48:14,189:INFO:          ipywidgets: 8.1.1
2024-04-22 10:48:14,189:INFO:                tqdm: 4.66.1
2024-04-22 10:48:14,189:INFO:               numpy: 1.25.2
2024-04-22 10:48:14,189:INFO:              pandas: 1.5.3
2024-04-22 10:48:14,189:INFO:              jinja2: 3.1.2
2024-04-22 10:48:14,189:INFO:               scipy: 1.10.1
2024-04-22 10:48:14,189:INFO:              joblib: 1.3.2
2024-04-22 10:48:14,189:INFO:             sklearn: 1.2.2
2024-04-22 10:48:14,189:INFO:                pyod: 1.1.3
2024-04-22 10:48:14,189:INFO:            imblearn: 0.12.0
2024-04-22 10:48:14,189:INFO:   category_encoders: 2.6.3
2024-04-22 10:48:14,189:INFO:            lightgbm: 4.3.0
2024-04-22 10:48:14,189:INFO:               numba: 0.58.1
2024-04-22 10:48:14,189:INFO:            requests: 2.31.0
2024-04-22 10:48:14,189:INFO:          matplotlib: 3.6.0
2024-04-22 10:48:14,189:INFO:          scikitplot: 0.3.7
2024-04-22 10:48:14,189:INFO:         yellowbrick: 1.5
2024-04-22 10:48:14,189:INFO:              plotly: 5.19.0
2024-04-22 10:48:14,189:INFO:    plotly-resampler: Not installed
2024-04-22 10:48:14,189:INFO:             kaleido: 0.2.1
2024-04-22 10:48:14,189:INFO:           schemdraw: 0.15
2024-04-22 10:48:14,189:INFO:         statsmodels: 0.14.0
2024-04-22 10:48:14,189:INFO:              sktime: 0.21.1
2024-04-22 10:48:14,189:INFO:               tbats: 1.1.3
2024-04-22 10:48:14,189:INFO:            pmdarima: 2.0.4
2024-04-22 10:48:14,189:INFO:              psutil: 5.9.6
2024-04-22 10:48:14,189:INFO:          markupsafe: 2.1.3
2024-04-22 10:48:14,192:INFO:             pickle5: Not installed
2024-04-22 10:48:14,192:INFO:         cloudpickle: 2.2.1
2024-04-22 10:48:14,192:INFO:         deprecation: 2.1.0
2024-04-22 10:48:14,192:INFO:              xxhash: 3.4.1
2024-04-22 10:48:14,192:INFO:           wurlitzer: Not installed
2024-04-22 10:48:14,192:INFO:PyCaret optional dependencies:
2024-04-22 10:48:23,064:INFO:                shap: 0.44.1
2024-04-22 10:48:23,064:INFO:           interpret: 0.5.1
2024-04-22 10:48:23,064:INFO:                umap: 0.5.5
2024-04-22 10:48:23,064:INFO:     ydata_profiling: 4.6.0
2024-04-22 10:48:23,064:INFO:  explainerdashboard: 0.4.5
2024-04-22 10:48:23,064:INFO:             autoviz: Not installed
2024-04-22 10:48:23,064:INFO:           fairlearn: 0.7.0
2024-04-22 10:48:23,064:INFO:          deepchecks: Not installed
2024-04-22 10:48:23,064:INFO:             xgboost: Not installed
2024-04-22 10:48:23,064:INFO:            catboost: 1.2.2
2024-04-22 10:48:23,064:INFO:              kmodes: 0.12.2
2024-04-22 10:48:23,066:INFO:             mlxtend: 0.23.1
2024-04-22 10:48:23,066:INFO:       statsforecast: 1.5.0
2024-04-22 10:48:23,066:INFO:        tune_sklearn: Not installed
2024-04-22 10:48:23,066:INFO:                 ray: Not installed
2024-04-22 10:48:23,066:INFO:            hyperopt: 0.2.7
2024-04-22 10:48:23,066:INFO:              optuna: 3.5.0
2024-04-22 10:48:23,066:INFO:               skopt: 0.9.0
2024-04-22 10:48:23,066:INFO:              mlflow: 1.30.1
2024-04-22 10:48:23,066:INFO:              gradio: 3.50.2
2024-04-22 10:48:23,066:INFO:             fastapi: 0.109.2
2024-04-22 10:48:23,066:INFO:             uvicorn: 0.27.1
2024-04-22 10:48:23,066:INFO:              m2cgen: 0.10.0
2024-04-22 10:48:23,066:INFO:           evidently: 0.2.8
2024-04-22 10:48:23,066:INFO:               fugue: 0.8.6
2024-04-22 10:48:23,066:INFO:           streamlit: 1.33.0
2024-04-22 10:48:23,066:INFO:             prophet: Not installed
2024-04-22 10:48:23,066:INFO:None
2024-04-22 10:48:23,066:INFO:Set up data.
2024-04-22 10:48:23,109:INFO:Set up folding strategy.
2024-04-22 10:48:23,109:INFO:Set up train/test split.
2024-04-22 10:48:23,129:INFO:Set up index.
2024-04-22 10:48:23,134:INFO:Assigning column types.
2024-04-22 10:48:23,138:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-04-22 10:48:23,286:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-04-22 10:48:23,292:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-22 10:48:23,386:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:48:23,386:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:48:24,158:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-04-22 10:48:24,158:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-22 10:48:24,242:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:48:24,242:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:48:24,244:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-04-22 10:48:24,339:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-22 10:48:24,423:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:48:24,423:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:48:24,607:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-22 10:48:24,753:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:48:24,756:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:48:24,757:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-04-22 10:48:25,180:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:48:25,183:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:48:25,434:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:48:25,434:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:48:25,448:INFO:Preparing preprocessing pipeline...
2024-04-22 10:48:25,457:INFO:Set up simple imputation.
2024-04-22 10:48:25,457:INFO:Set up column name cleaning.
2024-04-22 10:48:25,528:INFO:Finished creating preprocessing pipeline.
2024-04-22 10:48:25,539:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\BAGAS\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strate...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-04-22 10:48:25,539:INFO:Creating final display dataframe.
2024-04-22 10:48:26,401:INFO:Setup _display_container:                     Description             Value
0                    Session id                69
1                        Target             happy
2                   Target type            Binary
3           Original data shape          (143, 7)
4        Transformed data shape          (143, 7)
5   Transformed train set shape          (100, 7)
6    Transformed test set shape           (43, 7)
7              Numeric features                 6
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              779d
2024-04-22 10:48:26,841:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:48:26,841:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:48:26,985:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:48:26,985:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:48:26,988:INFO:setup() successfully completed in 15.17s...............
2024-04-22 10:48:29,270:INFO:Initializing compare_models()
2024-04-22 10:48:29,271:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-04-22 10:48:29,271:INFO:Checking exceptions
2024-04-22 10:48:29,283:INFO:Preparing display monitor
2024-04-22 10:48:29,400:INFO:Initializing Logistic Regression
2024-04-22 10:48:29,400:INFO:Total runtime is 1.6780694325764976e-05 minutes
2024-04-22 10:48:29,405:INFO:SubProcess create_model() called ==================================
2024-04-22 10:48:29,407:INFO:Initializing create_model()
2024-04-22 10:48:29,408:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290AB674AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:48:29,408:INFO:Checking exceptions
2024-04-22 10:48:29,408:INFO:Importing libraries
2024-04-22 10:48:29,408:INFO:Copying training dataset
2024-04-22 10:48:29,416:INFO:Defining folds
2024-04-22 10:48:29,416:INFO:Declaring metric variables
2024-04-22 10:48:29,426:INFO:Importing untrained model
2024-04-22 10:48:29,433:INFO:Logistic Regression Imported successfully
2024-04-22 10:48:29,443:INFO:Starting cross validation
2024-04-22 10:48:29,469:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:49:32,316:INFO:Calculating mean and std
2024-04-22 10:49:32,455:INFO:Creating metrics dataframe
2024-04-22 10:49:32,622:INFO:Uploading results into container
2024-04-22 10:49:32,636:INFO:Uploading model into container now
2024-04-22 10:49:32,653:INFO:_master_model_container: 1
2024-04-22 10:49:32,653:INFO:_display_container: 2
2024-04-22 10:49:32,663:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=69, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-04-22 10:49:32,663:INFO:create_model() successfully completed......................................
2024-04-22 10:49:41,007:INFO:SubProcess create_model() end ==================================
2024-04-22 10:49:41,009:INFO:Creating metrics dataframe
2024-04-22 10:49:41,213:INFO:Initializing K Neighbors Classifier
2024-04-22 10:49:41,213:INFO:Total runtime is 1.1969166358311971 minutes
2024-04-22 10:49:41,229:INFO:SubProcess create_model() called ==================================
2024-04-22 10:49:41,229:INFO:Initializing create_model()
2024-04-22 10:49:41,232:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290AB674AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:49:41,238:INFO:Checking exceptions
2024-04-22 10:49:41,238:INFO:Importing libraries
2024-04-22 10:49:41,245:INFO:Copying training dataset
2024-04-22 10:49:41,296:INFO:Defining folds
2024-04-22 10:49:41,302:INFO:Declaring metric variables
2024-04-22 10:49:41,313:INFO:Importing untrained model
2024-04-22 10:49:41,328:INFO:K Neighbors Classifier Imported successfully
2024-04-22 10:49:41,346:INFO:Starting cross validation
2024-04-22 10:49:41,403:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:49:44,727:INFO:Calculating mean and std
2024-04-22 10:49:44,733:INFO:Creating metrics dataframe
2024-04-22 10:49:44,738:INFO:Uploading results into container
2024-04-22 10:49:44,738:INFO:Uploading model into container now
2024-04-22 10:49:44,743:INFO:_master_model_container: 2
2024-04-22 10:49:44,743:INFO:_display_container: 2
2024-04-22 10:49:44,743:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-04-22 10:49:44,743:INFO:create_model() successfully completed......................................
2024-04-22 10:49:44,971:INFO:SubProcess create_model() end ==================================
2024-04-22 10:49:44,971:INFO:Creating metrics dataframe
2024-04-22 10:49:45,003:INFO:Initializing Naive Bayes
2024-04-22 10:49:45,003:INFO:Total runtime is 1.2600773016611735 minutes
2024-04-22 10:49:45,009:INFO:SubProcess create_model() called ==================================
2024-04-22 10:49:45,009:INFO:Initializing create_model()
2024-04-22 10:49:45,010:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290AB674AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:49:45,010:INFO:Checking exceptions
2024-04-22 10:49:45,010:INFO:Importing libraries
2024-04-22 10:49:45,011:INFO:Copying training dataset
2024-04-22 10:49:45,023:INFO:Defining folds
2024-04-22 10:49:45,023:INFO:Declaring metric variables
2024-04-22 10:49:45,028:INFO:Importing untrained model
2024-04-22 10:49:45,043:INFO:Naive Bayes Imported successfully
2024-04-22 10:49:45,062:INFO:Starting cross validation
2024-04-22 10:49:45,065:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:49:45,497:INFO:Calculating mean and std
2024-04-22 10:49:45,500:INFO:Creating metrics dataframe
2024-04-22 10:49:45,504:INFO:Uploading results into container
2024-04-22 10:49:45,505:INFO:Uploading model into container now
2024-04-22 10:49:45,507:INFO:_master_model_container: 3
2024-04-22 10:49:45,507:INFO:_display_container: 2
2024-04-22 10:49:45,507:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-04-22 10:49:45,507:INFO:create_model() successfully completed......................................
2024-04-22 10:49:45,788:INFO:SubProcess create_model() end ==================================
2024-04-22 10:49:45,788:INFO:Creating metrics dataframe
2024-04-22 10:49:45,845:INFO:Initializing Decision Tree Classifier
2024-04-22 10:49:45,846:INFO:Total runtime is 1.2741232236226399 minutes
2024-04-22 10:49:45,862:INFO:SubProcess create_model() called ==================================
2024-04-22 10:49:45,862:INFO:Initializing create_model()
2024-04-22 10:49:45,862:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290AB674AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:49:45,862:INFO:Checking exceptions
2024-04-22 10:49:45,862:INFO:Importing libraries
2024-04-22 10:49:45,863:INFO:Copying training dataset
2024-04-22 10:49:45,883:INFO:Defining folds
2024-04-22 10:49:45,885:INFO:Declaring metric variables
2024-04-22 10:49:45,891:INFO:Importing untrained model
2024-04-22 10:49:45,901:INFO:Decision Tree Classifier Imported successfully
2024-04-22 10:49:45,913:INFO:Starting cross validation
2024-04-22 10:49:45,919:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:49:46,719:INFO:Calculating mean and std
2024-04-22 10:49:46,720:INFO:Creating metrics dataframe
2024-04-22 10:49:46,725:INFO:Uploading results into container
2024-04-22 10:49:46,726:INFO:Uploading model into container now
2024-04-22 10:49:46,727:INFO:_master_model_container: 4
2024-04-22 10:49:46,727:INFO:_display_container: 2
2024-04-22 10:49:46,728:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=69, splitter='best')
2024-04-22 10:49:46,730:INFO:create_model() successfully completed......................................
2024-04-22 10:49:46,922:INFO:SubProcess create_model() end ==================================
2024-04-22 10:49:46,922:INFO:Creating metrics dataframe
2024-04-22 10:49:47,190:INFO:Initializing SVM - Linear Kernel
2024-04-22 10:49:47,190:INFO:Total runtime is 1.296527103583018 minutes
2024-04-22 10:49:47,200:INFO:SubProcess create_model() called ==================================
2024-04-22 10:49:47,200:INFO:Initializing create_model()
2024-04-22 10:49:47,200:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290AB674AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:49:47,200:INFO:Checking exceptions
2024-04-22 10:49:47,200:INFO:Importing libraries
2024-04-22 10:49:47,200:INFO:Copying training dataset
2024-04-22 10:49:47,208:INFO:Defining folds
2024-04-22 10:49:47,208:INFO:Declaring metric variables
2024-04-22 10:49:47,219:INFO:Importing untrained model
2024-04-22 10:49:47,219:INFO:SVM - Linear Kernel Imported successfully
2024-04-22 10:49:47,232:INFO:Starting cross validation
2024-04-22 10:49:47,232:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:49:48,721:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:49:48,721:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:49:48,730:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:49:48,754:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:49:48,769:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:49:48,772:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:49:48,776:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:49:48,798:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:49:48,807:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:49:48,811:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:49:48,815:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:49:48,827:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:49:48,849:INFO:Calculating mean and std
2024-04-22 10:49:48,849:INFO:Creating metrics dataframe
2024-04-22 10:49:48,853:INFO:Uploading results into container
2024-04-22 10:49:48,853:INFO:Uploading model into container now
2024-04-22 10:49:48,853:INFO:_master_model_container: 5
2024-04-22 10:49:48,853:INFO:_display_container: 2
2024-04-22 10:49:48,853:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=69, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-04-22 10:49:48,853:INFO:create_model() successfully completed......................................
2024-04-22 10:49:49,005:INFO:SubProcess create_model() end ==================================
2024-04-22 10:49:49,005:INFO:Creating metrics dataframe
2024-04-22 10:49:49,019:INFO:Initializing Ridge Classifier
2024-04-22 10:49:49,019:INFO:Total runtime is 1.327001909414927 minutes
2024-04-22 10:49:49,021:INFO:SubProcess create_model() called ==================================
2024-04-22 10:49:49,021:INFO:Initializing create_model()
2024-04-22 10:49:49,021:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290AB674AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:49:49,021:INFO:Checking exceptions
2024-04-22 10:49:49,021:INFO:Importing libraries
2024-04-22 10:49:49,021:INFO:Copying training dataset
2024-04-22 10:49:49,028:INFO:Defining folds
2024-04-22 10:49:49,028:INFO:Declaring metric variables
2024-04-22 10:49:49,031:INFO:Importing untrained model
2024-04-22 10:49:49,037:INFO:Ridge Classifier Imported successfully
2024-04-22 10:49:49,041:INFO:Starting cross validation
2024-04-22 10:49:49,041:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:49:49,115:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:49:49,118:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:49:49,125:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:49:49,141:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:49:49,165:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:49:49,172:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:49:49,175:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:49:49,181:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:49:49,205:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:49:49,205:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:49:49,226:INFO:Calculating mean and std
2024-04-22 10:49:49,228:INFO:Creating metrics dataframe
2024-04-22 10:49:49,232:INFO:Uploading results into container
2024-04-22 10:49:49,239:INFO:Uploading model into container now
2024-04-22 10:49:49,239:INFO:_master_model_container: 6
2024-04-22 10:49:49,239:INFO:_display_container: 2
2024-04-22 10:49:49,239:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=69, solver='auto',
                tol=0.0001)
2024-04-22 10:49:49,239:INFO:create_model() successfully completed......................................
2024-04-22 10:49:49,381:INFO:SubProcess create_model() end ==================================
2024-04-22 10:49:49,381:INFO:Creating metrics dataframe
2024-04-22 10:49:49,392:INFO:Initializing Random Forest Classifier
2024-04-22 10:49:49,392:INFO:Total runtime is 1.333224054177602 minutes
2024-04-22 10:49:49,399:INFO:SubProcess create_model() called ==================================
2024-04-22 10:49:49,399:INFO:Initializing create_model()
2024-04-22 10:49:49,399:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290AB674AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:49:49,399:INFO:Checking exceptions
2024-04-22 10:49:49,399:INFO:Importing libraries
2024-04-22 10:49:49,399:INFO:Copying training dataset
2024-04-22 10:49:49,407:INFO:Defining folds
2024-04-22 10:49:49,407:INFO:Declaring metric variables
2024-04-22 10:49:49,408:INFO:Importing untrained model
2024-04-22 10:49:49,415:INFO:Random Forest Classifier Imported successfully
2024-04-22 10:49:49,419:INFO:Starting cross validation
2024-04-22 10:49:49,419:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:49:51,315:INFO:Calculating mean and std
2024-04-22 10:49:51,317:INFO:Creating metrics dataframe
2024-04-22 10:49:51,323:INFO:Uploading results into container
2024-04-22 10:49:51,323:INFO:Uploading model into container now
2024-04-22 10:49:51,323:INFO:_master_model_container: 7
2024-04-22 10:49:51,323:INFO:_display_container: 2
2024-04-22 10:49:51,325:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=69, verbose=0, warm_start=False)
2024-04-22 10:49:51,325:INFO:create_model() successfully completed......................................
2024-04-22 10:49:51,553:INFO:SubProcess create_model() end ==================================
2024-04-22 10:49:51,553:INFO:Creating metrics dataframe
2024-04-22 10:49:51,570:INFO:Initializing Quadratic Discriminant Analysis
2024-04-22 10:49:51,570:INFO:Total runtime is 1.3695278008778888 minutes
2024-04-22 10:49:51,576:INFO:SubProcess create_model() called ==================================
2024-04-22 10:49:51,577:INFO:Initializing create_model()
2024-04-22 10:49:51,577:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290AB674AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:49:51,577:INFO:Checking exceptions
2024-04-22 10:49:51,577:INFO:Importing libraries
2024-04-22 10:49:51,577:INFO:Copying training dataset
2024-04-22 10:49:51,579:INFO:Defining folds
2024-04-22 10:49:51,579:INFO:Declaring metric variables
2024-04-22 10:49:51,591:INFO:Importing untrained model
2024-04-22 10:49:51,600:INFO:Quadratic Discriminant Analysis Imported successfully
2024-04-22 10:49:51,615:INFO:Starting cross validation
2024-04-22 10:49:51,617:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:49:51,905:INFO:Calculating mean and std
2024-04-22 10:49:51,905:INFO:Creating metrics dataframe
2024-04-22 10:49:51,919:INFO:Uploading results into container
2024-04-22 10:49:51,919:INFO:Uploading model into container now
2024-04-22 10:49:51,923:INFO:_master_model_container: 8
2024-04-22 10:49:51,923:INFO:_display_container: 2
2024-04-22 10:49:51,926:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-04-22 10:49:51,933:INFO:create_model() successfully completed......................................
2024-04-22 10:49:52,215:INFO:SubProcess create_model() end ==================================
2024-04-22 10:49:52,216:INFO:Creating metrics dataframe
2024-04-22 10:49:52,239:INFO:Initializing Ada Boost Classifier
2024-04-22 10:49:52,239:INFO:Total runtime is 1.3806757609049478 minutes
2024-04-22 10:49:52,243:INFO:SubProcess create_model() called ==================================
2024-04-22 10:49:52,244:INFO:Initializing create_model()
2024-04-22 10:49:52,244:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290AB674AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:49:52,244:INFO:Checking exceptions
2024-04-22 10:49:52,244:INFO:Importing libraries
2024-04-22 10:49:52,244:INFO:Copying training dataset
2024-04-22 10:49:52,251:INFO:Defining folds
2024-04-22 10:49:52,251:INFO:Declaring metric variables
2024-04-22 10:49:52,254:INFO:Importing untrained model
2024-04-22 10:49:52,257:INFO:Ada Boost Classifier Imported successfully
2024-04-22 10:49:52,264:INFO:Starting cross validation
2024-04-22 10:49:52,268:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:49:53,053:INFO:Calculating mean and std
2024-04-22 10:49:53,056:INFO:Creating metrics dataframe
2024-04-22 10:49:53,066:INFO:Uploading results into container
2024-04-22 10:49:53,067:INFO:Uploading model into container now
2024-04-22 10:49:53,067:INFO:_master_model_container: 9
2024-04-22 10:49:53,067:INFO:_display_container: 2
2024-04-22 10:49:53,068:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=69)
2024-04-22 10:49:53,068:INFO:create_model() successfully completed......................................
2024-04-22 10:49:53,263:INFO:SubProcess create_model() end ==================================
2024-04-22 10:49:53,263:INFO:Creating metrics dataframe
2024-04-22 10:49:53,281:INFO:Initializing Gradient Boosting Classifier
2024-04-22 10:49:53,281:INFO:Total runtime is 1.3980410496393838 minutes
2024-04-22 10:49:53,289:INFO:SubProcess create_model() called ==================================
2024-04-22 10:49:53,289:INFO:Initializing create_model()
2024-04-22 10:49:53,289:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290AB674AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:49:53,289:INFO:Checking exceptions
2024-04-22 10:49:53,289:INFO:Importing libraries
2024-04-22 10:49:53,289:INFO:Copying training dataset
2024-04-22 10:49:53,293:INFO:Defining folds
2024-04-22 10:49:53,293:INFO:Declaring metric variables
2024-04-22 10:49:53,299:INFO:Importing untrained model
2024-04-22 10:49:53,299:INFO:Gradient Boosting Classifier Imported successfully
2024-04-22 10:49:53,316:INFO:Starting cross validation
2024-04-22 10:49:53,318:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:49:54,036:INFO:Calculating mean and std
2024-04-22 10:49:54,038:INFO:Creating metrics dataframe
2024-04-22 10:49:54,045:INFO:Uploading results into container
2024-04-22 10:49:54,045:INFO:Uploading model into container now
2024-04-22 10:49:54,047:INFO:_master_model_container: 10
2024-04-22 10:49:54,047:INFO:_display_container: 2
2024-04-22 10:49:54,049:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=69, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-04-22 10:49:54,052:INFO:create_model() successfully completed......................................
2024-04-22 10:49:54,236:INFO:SubProcess create_model() end ==================================
2024-04-22 10:49:54,236:INFO:Creating metrics dataframe
2024-04-22 10:49:54,253:INFO:Initializing Linear Discriminant Analysis
2024-04-22 10:49:54,253:INFO:Total runtime is 1.4142481446266173 minutes
2024-04-22 10:49:54,263:INFO:SubProcess create_model() called ==================================
2024-04-22 10:49:54,263:INFO:Initializing create_model()
2024-04-22 10:49:54,263:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290AB674AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:49:54,263:INFO:Checking exceptions
2024-04-22 10:49:54,263:INFO:Importing libraries
2024-04-22 10:49:54,263:INFO:Copying training dataset
2024-04-22 10:49:54,270:INFO:Defining folds
2024-04-22 10:49:54,270:INFO:Declaring metric variables
2024-04-22 10:49:54,283:INFO:Importing untrained model
2024-04-22 10:49:54,287:INFO:Linear Discriminant Analysis Imported successfully
2024-04-22 10:49:54,289:INFO:Starting cross validation
2024-04-22 10:49:54,295:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:49:54,473:INFO:Calculating mean and std
2024-04-22 10:49:54,474:INFO:Creating metrics dataframe
2024-04-22 10:49:54,474:INFO:Uploading results into container
2024-04-22 10:49:54,479:INFO:Uploading model into container now
2024-04-22 10:49:54,480:INFO:_master_model_container: 11
2024-04-22 10:49:54,481:INFO:_display_container: 2
2024-04-22 10:49:54,481:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-04-22 10:49:54,481:INFO:create_model() successfully completed......................................
2024-04-22 10:49:54,635:INFO:SubProcess create_model() end ==================================
2024-04-22 10:49:54,635:INFO:Creating metrics dataframe
2024-04-22 10:49:54,656:INFO:Initializing Extra Trees Classifier
2024-04-22 10:49:54,656:INFO:Total runtime is 1.4209643681844075 minutes
2024-04-22 10:49:54,663:INFO:SubProcess create_model() called ==================================
2024-04-22 10:49:54,663:INFO:Initializing create_model()
2024-04-22 10:49:54,663:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290AB674AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:49:54,663:INFO:Checking exceptions
2024-04-22 10:49:54,663:INFO:Importing libraries
2024-04-22 10:49:54,663:INFO:Copying training dataset
2024-04-22 10:49:54,668:INFO:Defining folds
2024-04-22 10:49:54,668:INFO:Declaring metric variables
2024-04-22 10:49:54,668:INFO:Importing untrained model
2024-04-22 10:49:54,682:INFO:Extra Trees Classifier Imported successfully
2024-04-22 10:49:54,690:INFO:Starting cross validation
2024-04-22 10:49:54,690:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:49:56,486:INFO:Calculating mean and std
2024-04-22 10:49:56,499:INFO:Creating metrics dataframe
2024-04-22 10:49:56,506:INFO:Uploading results into container
2024-04-22 10:49:56,507:INFO:Uploading model into container now
2024-04-22 10:49:56,508:INFO:_master_model_container: 12
2024-04-22 10:49:56,508:INFO:_display_container: 2
2024-04-22 10:49:56,508:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=69, verbose=0, warm_start=False)
2024-04-22 10:49:56,508:INFO:create_model() successfully completed......................................
2024-04-22 10:49:56,931:INFO:SubProcess create_model() end ==================================
2024-04-22 10:49:56,931:INFO:Creating metrics dataframe
2024-04-22 10:49:56,963:INFO:Initializing Light Gradient Boosting Machine
2024-04-22 10:49:56,963:INFO:Total runtime is 1.459411597251892 minutes
2024-04-22 10:49:56,969:INFO:SubProcess create_model() called ==================================
2024-04-22 10:49:56,969:INFO:Initializing create_model()
2024-04-22 10:49:56,969:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290AB674AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:49:56,970:INFO:Checking exceptions
2024-04-22 10:49:56,970:INFO:Importing libraries
2024-04-22 10:49:56,970:INFO:Copying training dataset
2024-04-22 10:49:56,974:INFO:Defining folds
2024-04-22 10:49:56,975:INFO:Declaring metric variables
2024-04-22 10:49:57,024:INFO:Importing untrained model
2024-04-22 10:49:57,039:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:49:57,067:INFO:Starting cross validation
2024-04-22 10:49:57,069:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:49:59,470:INFO:Calculating mean and std
2024-04-22 10:49:59,472:INFO:Creating metrics dataframe
2024-04-22 10:49:59,480:INFO:Uploading results into container
2024-04-22 10:49:59,481:INFO:Uploading model into container now
2024-04-22 10:49:59,481:INFO:_master_model_container: 13
2024-04-22 10:49:59,481:INFO:_display_container: 2
2024-04-22 10:49:59,483:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:49:59,483:INFO:create_model() successfully completed......................................
2024-04-22 10:49:59,689:INFO:SubProcess create_model() end ==================================
2024-04-22 10:49:59,690:INFO:Creating metrics dataframe
2024-04-22 10:49:59,725:INFO:Initializing CatBoost Classifier
2024-04-22 10:49:59,725:INFO:Total runtime is 1.5054357528686522 minutes
2024-04-22 10:49:59,732:INFO:SubProcess create_model() called ==================================
2024-04-22 10:49:59,732:INFO:Initializing create_model()
2024-04-22 10:49:59,732:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290AB674AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:49:59,732:INFO:Checking exceptions
2024-04-22 10:49:59,732:INFO:Importing libraries
2024-04-22 10:49:59,733:INFO:Copying training dataset
2024-04-22 10:49:59,741:INFO:Defining folds
2024-04-22 10:49:59,741:INFO:Declaring metric variables
2024-04-22 10:49:59,747:INFO:Importing untrained model
2024-04-22 10:49:59,769:INFO:CatBoost Classifier Imported successfully
2024-04-22 10:49:59,781:INFO:Starting cross validation
2024-04-22 10:49:59,784:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:50:13,296:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\model_selection\_validation.py:378: FitFailedWarning: 
3 fits failed out of a total of 10.
The score on these train-test partitions for these parameters will be set to 0.0.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
3 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 275, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 68, in _fit_one
    transformer.fit(*args, **fit_params)
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\catboost\core.py", line 5100, in fit
    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\catboost\core.py", line 2319, in _fit
    self._train(
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\catboost\core.py", line 1723, in _train
    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)
  File "_catboost.pyx", line 4645, in _catboost._CatBoost._train
  File "_catboost.pyx", line 4694, in _catboost._CatBoost._train
_catboost.CatBoostError: C:/Go_Agent/pipelines/BuildMaster/catboost.git/catboost/libs/train_lib/dir_helper.cpp:20: Can't create train working dir: catboost_info

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2024-04-22 10:50:13,314:INFO:Calculating mean and std
2024-04-22 10:50:14,196:INFO:Creating metrics dataframe
2024-04-22 10:50:15,097:INFO:Uploading results into container
2024-04-22 10:50:15,129:INFO:Uploading model into container now
2024-04-22 10:50:15,171:INFO:_master_model_container: 14
2024-04-22 10:50:15,171:INFO:_display_container: 2
2024-04-22 10:50:15,171:INFO:<catboost.core.CatBoostClassifier object at 0x00000290AB6E1710>
2024-04-22 10:50:15,172:INFO:create_model() successfully completed......................................
2024-04-22 10:50:28,902:INFO:SubProcess create_model() end ==================================
2024-04-22 10:50:28,911:INFO:Creating metrics dataframe
2024-04-22 10:50:29,091:INFO:Initializing Dummy Classifier
2024-04-22 10:50:29,091:INFO:Total runtime is 1.994883342583974 minutes
2024-04-22 10:50:29,104:INFO:SubProcess create_model() called ==================================
2024-04-22 10:50:29,107:INFO:Initializing create_model()
2024-04-22 10:50:29,107:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290AB674AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:50:29,108:INFO:Checking exceptions
2024-04-22 10:50:29,108:INFO:Importing libraries
2024-04-22 10:50:29,111:INFO:Copying training dataset
2024-04-22 10:50:29,121:INFO:Defining folds
2024-04-22 10:50:29,121:INFO:Declaring metric variables
2024-04-22 10:50:29,142:INFO:Importing untrained model
2024-04-22 10:50:29,147:INFO:Dummy Classifier Imported successfully
2024-04-22 10:50:29,158:INFO:Starting cross validation
2024-04-22 10:50:29,163:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:50:31,366:INFO:Calculating mean and std
2024-04-22 10:50:31,366:INFO:Creating metrics dataframe
2024-04-22 10:50:31,377:INFO:Uploading results into container
2024-04-22 10:50:31,378:INFO:Uploading model into container now
2024-04-22 10:50:31,378:INFO:_master_model_container: 15
2024-04-22 10:50:31,378:INFO:_display_container: 2
2024-04-22 10:50:31,379:INFO:DummyClassifier(constant=None, random_state=69, strategy='prior')
2024-04-22 10:50:31,379:INFO:create_model() successfully completed......................................
2024-04-22 10:50:31,578:INFO:SubProcess create_model() end ==================================
2024-04-22 10:50:31,578:INFO:Creating metrics dataframe
2024-04-22 10:50:31,619:INFO:Initializing create_model()
2024-04-22 10:50:31,619:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:50:31,619:INFO:Checking exceptions
2024-04-22 10:50:31,625:INFO:Importing libraries
2024-04-22 10:50:31,625:INFO:Copying training dataset
2024-04-22 10:50:31,625:INFO:Defining folds
2024-04-22 10:50:31,625:INFO:Declaring metric variables
2024-04-22 10:50:31,625:INFO:Importing untrained model
2024-04-22 10:50:31,625:INFO:Declaring custom model
2024-04-22 10:50:31,635:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:50:31,635:INFO:Cross validation set to False
2024-04-22 10:50:31,635:INFO:Fitting Model
2024-04-22 10:50:32,087:INFO:[LightGBM] [Info] Number of positive: 54, number of negative: 46
2024-04-22 10:50:32,112:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001216 seconds.
2024-04-22 10:50:32,112:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-04-22 10:50:32,112:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-04-22 10:50:32,112:INFO:[LightGBM] [Info] Total Bins 31
2024-04-22 10:50:32,126:INFO:[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 6
2024-04-22 10:50:32,143:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.540000 -> initscore=0.160343
2024-04-22 10:50:32,144:INFO:[LightGBM] [Info] Start training from score 0.160343
2024-04-22 10:50:32,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:32,280:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:50:32,280:INFO:create_model() successfully completed......................................
2024-04-22 10:50:32,749:INFO:_master_model_container: 15
2024-04-22 10:50:32,749:INFO:_display_container: 2
2024-04-22 10:50:32,749:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:50:32,749:INFO:compare_models() successfully completed......................................
2024-04-22 10:50:36,893:INFO:Initializing create_model()
2024-04-22 10:50:36,893:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:50:36,893:INFO:Checking exceptions
2024-04-22 10:50:36,920:INFO:Importing libraries
2024-04-22 10:50:36,920:INFO:Copying training dataset
2024-04-22 10:50:36,920:INFO:Defining folds
2024-04-22 10:50:36,920:INFO:Declaring metric variables
2024-04-22 10:50:36,928:INFO:Importing untrained model
2024-04-22 10:50:36,928:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:50:36,939:INFO:Starting cross validation
2024-04-22 10:50:36,939:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:50:38,802:INFO:Calculating mean and std
2024-04-22 10:50:38,804:INFO:Creating metrics dataframe
2024-04-22 10:50:38,814:INFO:Finalizing model
2024-04-22 10:50:38,828:INFO:[LightGBM] [Info] Number of positive: 54, number of negative: 46
2024-04-22 10:50:38,828:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000085 seconds.
2024-04-22 10:50:38,828:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-04-22 10:50:38,828:INFO:[LightGBM] [Info] Total Bins 31
2024-04-22 10:50:38,829:INFO:[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 6
2024-04-22 10:50:38,829:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.540000 -> initscore=0.160343
2024-04-22 10:50:38,829:INFO:[LightGBM] [Info] Start training from score 0.160343
2024-04-22 10:50:38,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:50:38,871:INFO:Uploading results into container
2024-04-22 10:50:38,878:INFO:Uploading model into container now
2024-04-22 10:50:38,888:INFO:_master_model_container: 16
2024-04-22 10:50:38,888:INFO:_display_container: 3
2024-04-22 10:50:38,893:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:50:38,893:INFO:create_model() successfully completed......................................
2024-04-22 10:50:39,079:INFO:Initializing tune_model()
2024-04-22 10:50:39,079:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-04-22 10:50:39,079:INFO:Checking exceptions
2024-04-22 10:50:39,099:INFO:Copying training dataset
2024-04-22 10:50:39,103:INFO:Checking base model
2024-04-22 10:50:39,103:INFO:Base model : Light Gradient Boosting Machine
2024-04-22 10:50:39,103:INFO:Declaring metric variables
2024-04-22 10:50:39,112:INFO:Defining Hyperparameters
2024-04-22 10:50:39,261:INFO:Tuning with n_jobs=-1
2024-04-22 10:50:39,261:INFO:Initializing RandomizedSearchCV
2024-04-22 10:51:31,760:INFO:best_params: {'actual_estimator__reg_lambda': 0.001, 'actual_estimator__reg_alpha': 1, 'actual_estimator__num_leaves': 10, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.7, 'actual_estimator__min_child_samples': 16, 'actual_estimator__learning_rate': 0.05, 'actual_estimator__feature_fraction': 0.7, 'actual_estimator__bagging_freq': 6, 'actual_estimator__bagging_fraction': 0.6}
2024-04-22 10:51:31,893:INFO:Hyperparameter search completed
2024-04-22 10:51:31,893:INFO:SubProcess create_model() called ==================================
2024-04-22 10:51:31,929:INFO:Initializing create_model()
2024-04-22 10:51:31,929:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000290A8BD5AD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 0.001, 'reg_alpha': 1, 'num_leaves': 10, 'n_estimators': 250, 'min_split_gain': 0.7, 'min_child_samples': 16, 'learning_rate': 0.05, 'feature_fraction': 0.7, 'bagging_freq': 6, 'bagging_fraction': 0.6})
2024-04-22 10:51:31,929:INFO:Checking exceptions
2024-04-22 10:51:31,938:INFO:Importing libraries
2024-04-22 10:51:31,943:INFO:Copying training dataset
2024-04-22 10:51:32,056:INFO:Defining folds
2024-04-22 10:51:32,058:INFO:Declaring metric variables
2024-04-22 10:51:32,145:INFO:Importing untrained model
2024-04-22 10:51:32,145:INFO:Declaring custom model
2024-04-22 10:51:32,160:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:51:32,181:INFO:Starting cross validation
2024-04-22 10:51:32,213:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:51:38,237:INFO:Calculating mean and std
2024-04-22 10:51:38,240:INFO:Creating metrics dataframe
2024-04-22 10:51:38,274:INFO:Finalizing model
2024-04-22 10:51:38,379:INFO:[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7
2024-04-22 10:51:38,379:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-04-22 10:51:38,379:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-04-22 10:51:38,398:INFO:[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7
2024-04-22 10:51:38,398:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-04-22 10:51:38,398:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-04-22 10:51:38,398:INFO:[LightGBM] [Info] Number of positive: 54, number of negative: 46
2024-04-22 10:51:38,407:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000424 seconds.
2024-04-22 10:51:38,407:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-04-22 10:51:38,408:INFO:[LightGBM] [Info] Total Bins 31
2024-04-22 10:51:38,409:INFO:[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 6
2024-04-22 10:51:38,409:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.540000 -> initscore=0.160343
2024-04-22 10:51:38,409:INFO:[LightGBM] [Info] Start training from score 0.160343
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,425:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,441:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,441:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,441:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,442:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,442:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,442:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,442:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,442:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,442:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,442:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,443:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,444:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,444:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,444:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,444:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,444:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,444:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,444:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,445:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,445:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,445:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,445:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,445:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,445:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,445:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,446:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,446:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,446:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,446:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,446:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,446:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,447:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,448:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,450:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,452:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,455:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,455:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,455:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,455:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,455:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,455:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,456:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,456:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,456:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,456:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,456:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,456:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,457:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,457:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,457:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,458:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,458:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,458:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,458:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,458:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,458:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,458:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:38,459:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:38,499:INFO:Uploading results into container
2024-04-22 10:51:38,499:INFO:Uploading model into container now
2024-04-22 10:51:38,509:INFO:_master_model_container: 17
2024-04-22 10:51:38,509:INFO:_display_container: 4
2024-04-22 10:51:38,509:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:51:38,514:INFO:create_model() successfully completed......................................
2024-04-22 10:51:41,125:INFO:SubProcess create_model() end ==================================
2024-04-22 10:51:41,125:INFO:choose_better activated
2024-04-22 10:51:41,132:INFO:SubProcess create_model() called ==================================
2024-04-22 10:51:41,132:INFO:Initializing create_model()
2024-04-22 10:51:41,132:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:51:41,134:INFO:Checking exceptions
2024-04-22 10:51:41,147:INFO:Importing libraries
2024-04-22 10:51:41,147:INFO:Copying training dataset
2024-04-22 10:51:41,148:INFO:Defining folds
2024-04-22 10:51:41,148:INFO:Declaring metric variables
2024-04-22 10:51:41,148:INFO:Importing untrained model
2024-04-22 10:51:41,148:INFO:Declaring custom model
2024-04-22 10:51:41,153:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:51:41,153:INFO:Starting cross validation
2024-04-22 10:51:41,153:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:51:42,666:INFO:Calculating mean and std
2024-04-22 10:51:42,666:INFO:Creating metrics dataframe
2024-04-22 10:51:42,669:INFO:Finalizing model
2024-04-22 10:51:42,684:INFO:[LightGBM] [Info] Number of positive: 54, number of negative: 46
2024-04-22 10:51:42,684:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000080 seconds.
2024-04-22 10:51:42,684:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-04-22 10:51:42,684:INFO:[LightGBM] [Info] Total Bins 31
2024-04-22 10:51:42,684:INFO:[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 6
2024-04-22 10:51:42,684:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.540000 -> initscore=0.160343
2024-04-22 10:51:42,684:INFO:[LightGBM] [Info] Start training from score 0.160343
2024-04-22 10:51:42,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:42,743:INFO:Uploading results into container
2024-04-22 10:51:42,743:INFO:Uploading model into container now
2024-04-22 10:51:42,743:INFO:_master_model_container: 18
2024-04-22 10:51:42,743:INFO:_display_container: 5
2024-04-22 10:51:42,745:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:51:42,745:INFO:create_model() successfully completed......................................
2024-04-22 10:51:42,929:INFO:SubProcess create_model() end ==================================
2024-04-22 10:51:42,929:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.66
2024-04-22 10:51:42,929:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.68
2024-04-22 10:51:42,929:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-04-22 10:51:42,929:INFO:choose_better completed
2024-04-22 10:51:42,948:INFO:_master_model_container: 18
2024-04-22 10:51:42,948:INFO:_display_container: 4
2024-04-22 10:51:42,948:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:51:42,948:INFO:tune_model() successfully completed......................................
2024-04-22 10:51:43,127:INFO:Initializing evaluate_model()
2024-04-22 10:51:43,127:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-04-22 10:51:43,164:INFO:Initializing plot_model()
2024-04-22 10:51:43,168:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-04-22 10:51:43,168:INFO:Checking exceptions
2024-04-22 10:51:43,168:INFO:Preloading libraries
2024-04-22 10:51:43,173:INFO:Copying training dataset
2024-04-22 10:51:43,173:INFO:Plot type: pipeline
2024-04-22 10:51:43,539:INFO:Visual Rendered Successfully
2024-04-22 10:51:43,675:INFO:plot_model() successfully completed......................................
2024-04-22 10:51:43,678:INFO:Initializing finalize_model()
2024-04-22 10:51:43,678:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-04-22 10:51:43,678:INFO:Finalizing LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:51:43,678:INFO:Initializing create_model()
2024-04-22 10:51:43,678:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000290A6CF5110>, estimator=LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:51:43,678:INFO:Checking exceptions
2024-04-22 10:51:43,678:INFO:Importing libraries
2024-04-22 10:51:43,678:INFO:Copying training dataset
2024-04-22 10:51:43,678:INFO:Defining folds
2024-04-22 10:51:43,678:INFO:Declaring metric variables
2024-04-22 10:51:43,678:INFO:Importing untrained model
2024-04-22 10:51:43,678:INFO:Declaring custom model
2024-04-22 10:51:43,689:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:51:43,689:INFO:Cross validation set to False
2024-04-22 10:51:43,689:INFO:Fitting Model
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-04-22 10:51:43,699:INFO:[LightGBM] [Info] Number of positive: 77, number of negative: 66
2024-04-22 10:51:43,699:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000046 seconds.
2024-04-22 10:51:43,699:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-04-22 10:51:43,699:INFO:[LightGBM] [Info] Total Bins 33
2024-04-22 10:51:43,699:INFO:[LightGBM] [Info] Number of data points in the train set: 143, number of used features: 6
2024-04-22 10:51:43,699:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.538462 -> initscore=0.154151
2024-04-22 10:51:43,699:INFO:[LightGBM] [Info] Start training from score 0.154151
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,710:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,710:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,713:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,713:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,713:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,713:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,713:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,714:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,714:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,715:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,715:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,715:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,716:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,717:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,717:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,717:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,718:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,719:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,720:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,721:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,721:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,721:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,721:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,722:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,723:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,723:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,723:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,723:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,723:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,725:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,728:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,728:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,734:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,749:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,754:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,759:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,761:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,768:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,768:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,768:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,769:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,769:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,769:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:51:43,769:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:51:43,779:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose='deprecated'))),
                ('cate...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.7,
                                importance_type='split', learning_rate=0.05,
                                max_depth=-1, min_child_samples=16,
                                min_child_weight=0.001, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                objective=None, random_state=69, reg_alpha=1,
                                reg_lambda=0.001, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-04-22 10:51:43,779:INFO:create_model() successfully completed......................................
2024-04-22 10:51:43,930:INFO:_master_model_container: 18
2024-04-22 10:51:43,930:INFO:_display_container: 4
2024-04-22 10:51:43,939:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose='deprecated'))),
                ('cate...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.7,
                                importance_type='split', learning_rate=0.05,
                                max_depth=-1, min_child_samples=16,
                                min_child_weight=0.001, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                objective=None, random_state=69, reg_alpha=1,
                                reg_lambda=0.001, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-04-22 10:51:43,939:INFO:finalize_model() successfully completed......................................
2024-04-22 10:51:44,101:INFO:Initializing save_model()
2024-04-22 10:51:44,101:INFO:save_model(model=LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=happiness_pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\BAGAS\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strate...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-04-22 10:51:44,101:INFO:Adding model into prep_pipe
2024-04-22 10:51:44,119:INFO:happiness_pipeline.pkl saved in current working directory
2024-04-22 10:51:44,129:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose='deprecated'))),
                ('cate...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.7,
                                importance_type='split', learning_rate=0.05,
                                max_depth=-1, min_child_samples=16,
                                min_child_weight=0.001, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                objective=None, random_state=69, reg_alpha=1,
                                reg_lambda=0.001, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-04-22 10:51:44,129:INFO:save_model() successfully completed......................................
2024-04-22 11:22:56,244:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 11:22:56,244:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 11:22:56,244:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 11:22:56,244:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 11:23:01,340:INFO:Initializing load_model()
2024-04-22 11:23:01,340:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 11:24:55,312:INFO:Initializing load_model()
2024-04-22 11:24:55,343:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 11:38:54,319:INFO:Initializing load_model()
2024-04-22 11:38:54,336:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 11:41:42,104:INFO:Initializing load_model()
2024-04-22 11:41:42,178:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 11:44:32,397:INFO:Initializing load_model()
2024-04-22 11:44:32,414:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 11:44:45,076:INFO:Initializing load_model()
2024-04-22 11:44:45,077:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 11:44:46,124:INFO:Initializing load_model()
2024-04-22 11:44:46,125:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 11:44:47,855:INFO:Initializing load_model()
2024-04-22 11:44:47,856:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 11:44:48,331:INFO:Initializing load_model()
2024-04-22 11:44:48,334:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 11:44:49,293:INFO:Initializing load_model()
2024-04-22 11:44:49,293:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 11:44:49,389:INFO:Initializing predict_model()
2024-04-22 11:44:49,389:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBCDB00350>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBB56D8F40>)
2024-04-22 11:44:49,389:INFO:Checking exceptions
2024-04-22 11:44:49,389:INFO:Preloading libraries
2024-04-22 11:44:49,395:INFO:Set up data.
2024-04-22 11:44:49,415:INFO:Set up index.
2024-04-22 11:47:44,278:INFO:Initializing load_model()
2024-04-22 11:47:44,305:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 11:48:45,860:INFO:Initializing load_model()
2024-04-22 11:48:45,870:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 11:49:13,155:INFO:Initializing load_model()
2024-04-22 11:49:13,190:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 11:49:14,625:INFO:Initializing predict_model()
2024-04-22 11:49:14,625:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBCDB02890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBC99E0CC0>)
2024-04-22 11:49:14,630:INFO:Checking exceptions
2024-04-22 11:49:14,630:INFO:Preloading libraries
2024-04-22 11:49:14,650:INFO:Set up data.
2024-04-22 11:49:14,790:INFO:Set up index.
2024-04-22 11:51:43,999:INFO:Initializing load_model()
2024-04-22 11:51:44,010:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 11:52:30,929:INFO:Initializing load_model()
2024-04-22 11:52:30,938:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:09:05,934:INFO:Initializing load_model()
2024-04-22 12:09:05,968:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:09:06,993:INFO:Initializing predict_model()
2024-04-22 12:09:06,994:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBCAB08DD0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBCDF472E0>)
2024-04-22 12:09:06,997:INFO:Checking exceptions
2024-04-22 12:09:06,997:INFO:Preloading libraries
2024-04-22 12:09:07,034:INFO:Set up data.
2024-04-22 12:09:07,187:INFO:Set up index.
2024-04-22 12:15:09,885:INFO:Initializing load_model()
2024-04-22 12:15:09,894:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:15:10,600:INFO:Initializing predict_model()
2024-04-22 12:15:10,605:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBB53F3E10>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBB56D8F40>)
2024-04-22 12:15:10,606:INFO:Checking exceptions
2024-04-22 12:15:10,606:INFO:Preloading libraries
2024-04-22 12:15:10,639:INFO:Set up data.
2024-04-22 12:15:10,834:INFO:Set up index.
2024-04-22 12:15:16,585:INFO:Initializing load_model()
2024-04-22 12:15:16,586:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:15:27,572:INFO:Initializing load_model()
2024-04-22 12:15:27,572:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:15:27,958:INFO:Initializing predict_model()
2024-04-22 12:15:27,959:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBCDC36490>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBCD8553A0>)
2024-04-22 12:15:27,961:INFO:Checking exceptions
2024-04-22 12:15:27,962:INFO:Preloading libraries
2024-04-22 12:15:27,978:INFO:Set up data.
2024-04-22 12:15:28,020:INFO:Set up index.
2024-04-22 12:17:44,109:INFO:Initializing load_model()
2024-04-22 12:17:44,241:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:18:13,094:INFO:Initializing load_model()
2024-04-22 12:18:13,100:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:18:20,632:INFO:Initializing load_model()
2024-04-22 12:18:20,633:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:18:20,716:INFO:Initializing predict_model()
2024-04-22 12:18:20,716:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBCD516150>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBCDF468E0>)
2024-04-22 12:18:20,718:INFO:Checking exceptions
2024-04-22 12:18:20,718:INFO:Preloading libraries
2024-04-22 12:18:20,724:INFO:Set up data.
2024-04-22 12:18:20,742:INFO:Set up index.
2024-04-22 12:19:08,095:INFO:Initializing load_model()
2024-04-22 12:19:08,115:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:16,267:INFO:Initializing load_model()
2024-04-22 12:19:16,267:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:16,372:INFO:Initializing predict_model()
2024-04-22 12:19:16,372:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBCDB14850>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBB54036A0>)
2024-04-22 12:19:16,373:INFO:Checking exceptions
2024-04-22 12:19:16,373:INFO:Preloading libraries
2024-04-22 12:19:16,379:INFO:Set up data.
2024-04-22 12:19:16,396:INFO:Set up index.
2024-04-22 12:19:33,427:INFO:Initializing load_model()
2024-04-22 12:19:33,441:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:37,778:INFO:Initializing load_model()
2024-04-22 12:19:37,780:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:38,842:INFO:Initializing load_model()
2024-04-22 12:19:38,842:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:39,246:INFO:Initializing load_model()
2024-04-22 12:19:39,252:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:39,722:INFO:Initializing load_model()
2024-04-22 12:19:39,722:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:40,699:INFO:Initializing load_model()
2024-04-22 12:19:40,699:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:42,913:INFO:Initializing load_model()
2024-04-22 12:19:42,913:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:42,974:INFO:Initializing predict_model()
2024-04-22 12:19:42,974:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBCDB136D0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBCD8D60C0>)
2024-04-22 12:19:42,974:INFO:Checking exceptions
2024-04-22 12:19:42,974:INFO:Preloading libraries
2024-04-22 12:19:42,982:INFO:Set up data.
2024-04-22 12:19:42,996:INFO:Set up index.
2024-04-22 12:19:49,373:INFO:Initializing load_model()
2024-04-22 12:19:49,373:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:49,877:INFO:Initializing load_model()
2024-04-22 12:19:49,879:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:50,381:INFO:Initializing load_model()
2024-04-22 12:19:50,381:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:50,822:INFO:Initializing load_model()
2024-04-22 12:19:50,822:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:51,239:INFO:Initializing load_model()
2024-04-22 12:19:51,239:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:52,552:INFO:Initializing load_model()
2024-04-22 12:19:52,552:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:53,454:INFO:Initializing load_model()
2024-04-22 12:19:53,454:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:19:53,575:INFO:Initializing predict_model()
2024-04-22 12:19:53,576:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBCDB0A9D0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBCD246D40>)
2024-04-22 12:19:53,578:INFO:Checking exceptions
2024-04-22 12:19:53,578:INFO:Preloading libraries
2024-04-22 12:19:53,582:INFO:Set up data.
2024-04-22 12:19:53,604:INFO:Set up index.
2024-04-22 12:20:29,288:INFO:Initializing load_model()
2024-04-22 12:20:29,303:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:20:48,586:INFO:Initializing load_model()
2024-04-22 12:20:48,588:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:20:48,710:INFO:Initializing predict_model()
2024-04-22 12:20:48,711:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBCDEE9550>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBCD246D40>)
2024-04-22 12:20:48,711:INFO:Checking exceptions
2024-04-22 12:20:48,711:INFO:Preloading libraries
2024-04-22 12:20:48,716:INFO:Set up data.
2024-04-22 12:20:48,726:INFO:Set up index.
2024-04-22 12:20:52,586:INFO:Initializing load_model()
2024-04-22 12:20:52,586:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:20:53,462:INFO:Initializing load_model()
2024-04-22 12:20:53,463:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:20:54,297:INFO:Initializing load_model()
2024-04-22 12:20:54,297:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:20:54,778:INFO:Initializing load_model()
2024-04-22 12:20:54,778:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:20:55,345:INFO:Initializing load_model()
2024-04-22 12:20:55,345:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:20:55,796:INFO:Initializing load_model()
2024-04-22 12:20:55,796:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:20:56,847:INFO:Initializing load_model()
2024-04-22 12:20:56,847:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:20:56,908:INFO:Initializing predict_model()
2024-04-22 12:20:56,909:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBCDE73E50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBCDF47600>)
2024-04-22 12:20:56,911:INFO:Checking exceptions
2024-04-22 12:20:56,911:INFO:Preloading libraries
2024-04-22 12:20:56,911:INFO:Set up data.
2024-04-22 12:20:56,917:INFO:Set up index.
2024-04-22 12:27:46,933:INFO:Initializing load_model()
2024-04-22 12:27:46,942:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:27:58,157:INFO:Initializing load_model()
2024-04-22 12:27:58,160:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:27:58,271:INFO:Initializing predict_model()
2024-04-22 12:27:58,273:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBCD59B190>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBCD8D5DA0>)
2024-04-22 12:27:58,274:INFO:Checking exceptions
2024-04-22 12:27:58,274:INFO:Preloading libraries
2024-04-22 12:27:58,284:INFO:Set up data.
2024-04-22 12:27:58,326:INFO:Set up index.
2024-04-22 12:28:03,557:INFO:Initializing load_model()
2024-04-22 12:28:03,557:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:28:03,787:INFO:Initializing load_model()
2024-04-22 12:28:03,787:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:28:05,651:INFO:Initializing load_model()
2024-04-22 12:28:05,651:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:28:06,410:INFO:Initializing load_model()
2024-04-22 12:28:06,412:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:28:07,274:INFO:Initializing load_model()
2024-04-22 12:28:07,274:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:28:07,873:INFO:Initializing load_model()
2024-04-22 12:28:07,875:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:28:08,525:INFO:Initializing load_model()
2024-04-22 12:28:08,525:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:28:08,667:INFO:Initializing predict_model()
2024-04-22 12:28:08,667:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBCDB04190>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBCD246B60>)
2024-04-22 12:28:08,679:INFO:Checking exceptions
2024-04-22 12:28:08,679:INFO:Preloading libraries
2024-04-22 12:28:08,682:INFO:Set up data.
2024-04-22 12:28:08,690:INFO:Set up index.
2024-04-22 12:35:46,065:INFO:Initializing load_model()
2024-04-22 12:35:46,084:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:36:45,309:INFO:Initializing load_model()
2024-04-22 12:36:45,329:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:37:12,929:INFO:Initializing load_model()
2024-04-22 12:37:12,933:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:37:47,167:INFO:Initializing load_model()
2024-04-22 12:37:47,183:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:37:53,106:INFO:Initializing load_model()
2024-04-22 12:37:53,106:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:37:53,231:INFO:Initializing predict_model()
2024-04-22 12:37:53,234:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBCDB0CE50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBCDF47100>)
2024-04-22 12:37:53,235:INFO:Checking exceptions
2024-04-22 12:37:53,235:INFO:Preloading libraries
2024-04-22 12:37:53,245:INFO:Set up data.
2024-04-22 12:37:53,263:INFO:Set up index.
2024-04-22 12:38:12,173:INFO:Initializing load_model()
2024-04-22 12:38:12,183:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:38:13,988:INFO:Initializing load_model()
2024-04-22 12:38:14,132:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:38:14,593:INFO:Initializing load_model()
2024-04-22 12:38:14,593:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:38:15,226:INFO:Initializing load_model()
2024-04-22 12:38:15,227:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:38:15,718:INFO:Initializing load_model()
2024-04-22 12:38:15,719:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:38:17,864:INFO:Initializing load_model()
2024-04-22 12:38:17,864:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:38:17,930:INFO:Initializing predict_model()
2024-04-22 12:38:17,930:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBCDE97610>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBCDF477E0>)
2024-04-22 12:38:17,930:INFO:Checking exceptions
2024-04-22 12:38:17,930:INFO:Preloading libraries
2024-04-22 12:38:17,939:INFO:Set up data.
2024-04-22 12:38:17,960:INFO:Set up index.
2024-04-22 12:42:17,638:INFO:Initializing load_model()
2024-04-22 12:42:17,640:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:43:04,033:INFO:Initializing load_model()
2024-04-22 12:43:04,055:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:43:12,903:INFO:Initializing load_model()
2024-04-22 12:43:12,949:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:43:13,177:INFO:Initializing predict_model()
2024-04-22 12:43:13,177:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBCDB16F10>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBCD8A9C60>)
2024-04-22 12:43:13,177:INFO:Checking exceptions
2024-04-22 12:43:13,177:INFO:Preloading libraries
2024-04-22 12:43:13,197:INFO:Set up data.
2024-04-22 12:43:13,229:INFO:Set up index.
2024-04-22 12:43:38,400:INFO:Initializing load_model()
2024-04-22 12:43:38,413:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:43:48,262:INFO:Initializing load_model()
2024-04-22 12:43:48,263:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 12:43:48,339:INFO:Initializing predict_model()
2024-04-22 12:43:48,339:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001CBBCCACCD0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001CBB5B96B60>)
2024-04-22 12:43:48,340:INFO:Checking exceptions
2024-04-22 12:43:48,340:INFO:Preloading libraries
2024-04-22 12:43:48,344:INFO:Set up data.
2024-04-22 12:43:48,360:INFO:Set up index.
2024-04-22 12:44:22,743:INFO:Initializing load_model()
2024-04-22 12:44:22,755:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 03:43:57,472:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-25 03:43:57,489:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-25 03:43:57,489:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-25 03:43:57,489:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-25 03:44:05,985:INFO:Initializing load_model()
2024-04-25 03:44:05,985:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 03:52:31,195:INFO:Initializing load_model()
2024-04-25 03:52:31,242:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 03:52:39,689:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-25 03:52:39,689:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-25 03:52:39,690:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-25 03:52:39,690:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-25 03:54:10,423:INFO:Initializing load_model()
2024-04-25 03:54:10,439:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 03:54:19,567:INFO:Initializing load_model()
2024-04-25 03:54:19,567:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 03:55:41,242:INFO:Initializing load_model()
2024-04-25 03:55:41,251:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 03:57:25,860:INFO:Initializing load_model()
2024-04-25 03:57:25,933:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 03:57:52,887:INFO:Initializing load_model()
2024-04-25 03:57:52,903:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:02:45,161:INFO:Initializing load_model()
2024-04-25 04:02:45,178:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:04:17,148:INFO:Initializing load_model()
2024-04-25 04:04:17,164:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:07:59,730:INFO:Initializing load_model()
2024-04-25 04:07:59,778:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:08:33,542:INFO:Initializing load_model()
2024-04-25 04:08:33,544:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:09:18,350:INFO:Initializing load_model()
2024-04-25 04:09:18,363:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:09:27,469:INFO:Initializing load_model()
2024-04-25 04:09:27,469:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:15:32,087:INFO:Initializing load_model()
2024-04-25 04:15:32,117:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:15:41,359:INFO:Initializing load_model()
2024-04-25 04:15:41,359:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:15:46,383:INFO:Initializing load_model()
2024-04-25 04:15:46,383:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:15:46,482:INFO:Initializing predict_model()
2024-04-25 04:15:46,482:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E7A7192F50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001E7A723DEE0>)
2024-04-25 04:15:46,482:INFO:Checking exceptions
2024-04-25 04:15:46,482:INFO:Preloading libraries
2024-04-25 04:15:46,488:INFO:Set up data.
2024-04-25 04:15:46,501:INFO:Set up index.
2024-04-25 04:16:14,735:INFO:Initializing load_model()
2024-04-25 04:16:14,742:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:16:20,490:INFO:Initializing load_model()
2024-04-25 04:16:20,491:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:16:20,539:INFO:Initializing predict_model()
2024-04-25 04:16:20,540:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E7A4E43550>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001E7A723F240>)
2024-04-25 04:16:20,540:INFO:Checking exceptions
2024-04-25 04:16:20,540:INFO:Preloading libraries
2024-04-25 04:16:20,541:INFO:Set up data.
2024-04-25 04:16:20,551:INFO:Set up index.
2024-04-25 04:16:25,367:INFO:Initializing load_model()
2024-04-25 04:16:25,367:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:16:26,591:INFO:Initializing load_model()
2024-04-25 04:16:26,591:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:16:28,095:INFO:Initializing load_model()
2024-04-25 04:16:28,095:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:16:28,487:INFO:Initializing load_model()
2024-04-25 04:16:28,487:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:16:31,223:INFO:Initializing load_model()
2024-04-25 04:16:31,223:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:16:31,697:INFO:Initializing load_model()
2024-04-25 04:16:31,697:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:16:32,761:INFO:Initializing load_model()
2024-04-25 04:16:32,761:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:16:32,803:INFO:Initializing predict_model()
2024-04-25 04:16:32,803:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E7A74BD890>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001E7A723CD60>)
2024-04-25 04:16:32,804:INFO:Checking exceptions
2024-04-25 04:16:32,804:INFO:Preloading libraries
2024-04-25 04:16:32,805:INFO:Set up data.
2024-04-25 04:16:32,807:INFO:Set up index.
2024-04-25 04:17:44,945:INFO:Initializing load_model()
2024-04-25 04:17:44,958:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:17:50,247:INFO:Initializing load_model()
2024-04-25 04:17:50,247:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:17:51,070:INFO:Initializing load_model()
2024-04-25 04:17:51,070:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:17:51,609:INFO:Initializing load_model()
2024-04-25 04:17:51,609:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:17:51,688:INFO:Initializing predict_model()
2024-04-25 04:17:51,688:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E7A7191DD0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001E7A723C4A0>)
2024-04-25 04:17:51,694:INFO:Checking exceptions
2024-04-25 04:17:51,694:INFO:Preloading libraries
2024-04-25 04:17:51,694:INFO:Set up data.
2024-04-25 04:17:51,720:INFO:Set up index.
2024-04-25 04:17:58,581:INFO:Initializing load_model()
2024-04-25 04:17:58,581:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:18:50,239:INFO:Initializing load_model()
2024-04-25 04:18:50,242:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:25:15,954:INFO:Initializing load_model()
2024-04-25 04:25:15,968:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:28:35,195:INFO:Initializing load_model()
2024-04-25 04:28:35,225:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:29:09,656:INFO:Initializing load_model()
2024-04-25 04:29:09,656:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:29:12,914:INFO:Initializing load_model()
2024-04-25 04:29:12,914:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:29:16,943:INFO:Initializing load_model()
2024-04-25 04:29:16,943:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:30:16,339:INFO:Initializing load_model()
2024-04-25 04:30:16,355:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:30:36,568:INFO:Initializing load_model()
2024-04-25 04:30:36,575:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:31:47,741:INFO:Initializing load_model()
2024-04-25 04:31:47,750:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:35:24,981:INFO:Initializing load_model()
2024-04-25 04:35:25,021:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:36:14,722:INFO:Initializing load_model()
2024-04-25 04:36:14,753:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:36:20,620:INFO:Initializing load_model()
2024-04-25 04:36:20,620:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:36:20,677:INFO:Initializing predict_model()
2024-04-25 04:36:20,678:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E7A6C330D0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001E78E81A7A0>)
2024-04-25 04:36:20,678:INFO:Checking exceptions
2024-04-25 04:36:20,679:INFO:Preloading libraries
2024-04-25 04:36:20,684:INFO:Set up data.
2024-04-25 04:36:20,706:INFO:Set up index.
2024-04-25 04:41:17,774:INFO:Initializing load_model()
2024-04-25 04:41:17,798:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:41:23,928:INFO:Initializing load_model()
2024-04-25 04:41:23,929:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:41:24,043:INFO:Initializing predict_model()
2024-04-25 04:41:24,043:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E78E83C9D0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001E78CE7B560>)
2024-04-25 04:41:24,043:INFO:Checking exceptions
2024-04-25 04:41:24,043:INFO:Preloading libraries
2024-04-25 04:41:24,043:INFO:Set up data.
2024-04-25 04:41:24,073:INFO:Set up index.
2024-04-25 04:41:52,951:INFO:Initializing load_model()
2024-04-25 04:41:52,952:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:41:59,729:INFO:Initializing load_model()
2024-04-25 04:41:59,729:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:41:59,771:INFO:Initializing predict_model()
2024-04-25 04:41:59,775:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E7A723A590>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001E78E81A7A0>)
2024-04-25 04:41:59,775:INFO:Checking exceptions
2024-04-25 04:41:59,775:INFO:Preloading libraries
2024-04-25 04:41:59,780:INFO:Set up data.
2024-04-25 04:41:59,790:INFO:Set up index.
2024-04-25 04:43:04,206:INFO:Initializing load_model()
2024-04-25 04:43:04,222:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:43:40,826:INFO:Initializing load_model()
2024-04-25 04:43:40,832:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:43:57,784:INFO:Initializing load_model()
2024-04-25 04:43:57,790:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:44:12,220:INFO:Initializing load_model()
2024-04-25 04:44:12,220:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-25 04:46:12,577:INFO:Initializing load_model()
2024-04-25 04:46:12,589:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:38:35,406:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-27 12:38:35,418:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-27 12:38:35,418:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-27 12:38:35,418:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-27 12:38:44,941:INFO:Initializing load_model()
2024-04-27 12:38:44,941:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:42:00,479:INFO:Initializing load_model()
2024-04-27 12:42:00,495:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:43:13,091:INFO:Initializing load_model()
2024-04-27 12:43:13,101:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:45:33,296:INFO:Initializing load_model()
2024-04-27 12:45:33,325:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:45:49,528:INFO:Initializing load_model()
2024-04-27 12:45:49,528:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:45:52,931:INFO:Initializing load_model()
2024-04-27 12:45:52,931:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:46:31,061:INFO:Initializing load_model()
2024-04-27 12:46:31,071:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:46:57,048:INFO:Initializing load_model()
2024-04-27 12:46:57,059:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:48:04,797:INFO:Initializing load_model()
2024-04-27 12:48:04,802:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:48:21,873:INFO:Initializing load_model()
2024-04-27 12:48:21,873:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:50:14,161:INFO:Initializing load_model()
2024-04-27 12:50:14,170:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:51:30,284:INFO:Initializing load_model()
2024-04-27 12:51:30,300:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:52:08,845:INFO:Initializing load_model()
2024-04-27 12:52:08,854:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:52:26,019:INFO:Initializing load_model()
2024-04-27 12:52:26,019:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:52:26,082:INFO:Initializing predict_model()
2024-04-27 12:52:26,082:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021A78BDCB50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000021A78B407C0>)
2024-04-27 12:52:26,082:INFO:Checking exceptions
2024-04-27 12:52:26,082:INFO:Preloading libraries
2024-04-27 12:52:26,086:INFO:Set up data.
2024-04-27 12:52:26,095:INFO:Set up index.
2024-04-27 12:52:46,080:INFO:Initializing load_model()
2024-04-27 12:52:46,084:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:52:51,669:INFO:Initializing load_model()
2024-04-27 12:52:51,669:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 12:52:51,714:INFO:Initializing predict_model()
2024-04-27 12:52:51,714:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021A78711FD0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000021A78B66F20>)
2024-04-27 12:52:51,714:INFO:Checking exceptions
2024-04-27 12:52:51,714:INFO:Preloading libraries
2024-04-27 12:52:51,717:INFO:Set up data.
2024-04-27 12:52:51,722:INFO:Set up index.
2024-04-27 12:56:50,421:INFO:Initializing load_model()
2024-04-27 12:56:50,431:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 13:42:37,064:INFO:Initializing load_model()
2024-04-27 13:42:37,083:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 14:38:11,300:INFO:Initializing load_model()
2024-04-27 14:38:11,312:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 14:38:29,525:INFO:Initializing load_model()
2024-04-27 14:38:29,527:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 14:38:58,116:INFO:Initializing load_model()
2024-04-27 14:38:58,116:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 14:39:26,866:INFO:Initializing load_model()
2024-04-27 14:39:26,894:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 14:40:54,688:INFO:Initializing load_model()
2024-04-27 14:40:54,699:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 14:51:51,195:INFO:Initializing load_model()
2024-04-27 14:51:51,254:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 14:52:56,878:INFO:Initializing load_model()
2024-04-27 14:52:56,889:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 14:53:14,978:INFO:Initializing load_model()
2024-04-27 14:53:14,978:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 14:53:52,007:INFO:Initializing load_model()
2024-04-27 14:53:52,016:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 14:54:32,914:INFO:Initializing load_model()
2024-04-27 14:54:32,918:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 14:54:57,189:INFO:Initializing load_model()
2024-04-27 14:54:57,206:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 14:55:11,658:INFO:Initializing load_model()
2024-04-27 14:55:11,662:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 14:55:33,048:INFO:Initializing load_model()
2024-04-27 14:55:33,048:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 14:56:11,852:INFO:Initializing load_model()
2024-04-27 14:56:11,855:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 14:57:20,980:INFO:Initializing load_model()
2024-04-27 14:57:20,989:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 15:00:21,953:INFO:Initializing load_model()
2024-04-27 15:00:21,965:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 15:02:00,371:INFO:Initializing load_model()
2024-04-27 15:02:00,379:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 15:06:23,387:INFO:Initializing load_model()
2024-04-27 15:06:23,392:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 15:13:44,488:INFO:Initializing load_model()
2024-04-27 15:13:44,502:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 15:14:21,072:INFO:Initializing load_model()
2024-04-27 15:14:21,078:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 15:14:52,510:INFO:Initializing load_model()
2024-04-27 15:14:52,510:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 15:14:52,551:INFO:Initializing predict_model()
2024-04-27 15:14:52,551:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021A78AA6C50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000021A78B60F40>)
2024-04-27 15:14:52,553:INFO:Checking exceptions
2024-04-27 15:14:52,553:INFO:Preloading libraries
2024-04-27 15:14:52,555:INFO:Set up data.
2024-04-27 15:14:52,563:INFO:Set up index.
2024-04-27 15:15:38,219:INFO:Initializing load_model()
2024-04-27 15:15:38,228:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 15:15:56,992:INFO:Initializing load_model()
2024-04-27 15:15:56,992:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 15:16:03,559:INFO:Initializing load_model()
2024-04-27 15:16:03,559:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 15:16:07,880:INFO:Initializing load_model()
2024-04-27 15:16:07,881:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 15:16:07,946:INFO:Initializing predict_model()
2024-04-27 15:16:07,946:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021A78C1C250>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000021A6030FEC0>)
2024-04-27 15:16:07,946:INFO:Checking exceptions
2024-04-27 15:16:07,946:INFO:Preloading libraries
2024-04-27 15:16:07,961:INFO:Set up data.
2024-04-27 15:16:07,992:INFO:Set up index.
2024-04-27 15:16:26,137:INFO:Initializing load_model()
2024-04-27 15:16:26,154:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 15:16:26,456:INFO:Initializing predict_model()
2024-04-27 15:16:26,456:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021A602ED5D0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000021A78B4D6C0>)
2024-04-27 15:16:26,456:INFO:Checking exceptions
2024-04-27 15:16:26,456:INFO:Preloading libraries
2024-04-27 15:16:26,474:INFO:Set up data.
2024-04-27 15:16:26,497:INFO:Set up index.
2024-04-27 15:19:00,193:INFO:Initializing load_model()
2024-04-27 15:19:00,210:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 15:19:10,189:INFO:Initializing load_model()
2024-04-27 15:19:10,189:INFO:load_model(model_name=happiness_pipeline, platform=None, authentication=None, verbose=True)
2024-04-27 15:19:10,229:INFO:Initializing predict_model()
2024-04-27 15:19:10,229:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021A78BFD710>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'ëvents'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('trained_model',
                 LGBMClassifier(bagging_fraction=0.6, bagging_freq=6,
                                feature_fraction=0.7, learning_rate=0.05,
                                min_child_samples=16, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                random_state=69, reg_alpha=1,
                                reg_lambda=0.001))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000021A78B67420>)
2024-04-27 15:19:10,230:INFO:Checking exceptions
2024-04-27 15:19:10,230:INFO:Preloading libraries
2024-04-27 15:19:10,230:INFO:Set up data.
2024-04-27 15:19:10,235:INFO:Set up index.
