2024-04-22 10:04:10,280:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:04:10,280:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:04:10,280:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:04:10,280:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:04:14,389:INFO:Soft dependency imported: ydata_profiling: 4.6.0
2024-04-22 10:04:37,869:INFO:PyCaret ClassificationExperiment
2024-04-22 10:04:37,869:INFO:Logging name: clf-default-name
2024-04-22 10:04:37,869:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-04-22 10:04:37,869:INFO:version 3.2.0
2024-04-22 10:04:37,869:INFO:Initializing setup()
2024-04-22 10:04:37,869:INFO:self.USI: 0771
2024-04-22 10:04:37,869:INFO:self._variable_keys: {'exp_name_log', 'n_jobs_param', '_ml_usecase', 'USI', 'seed', 'fold_groups_param', 'target_param', 'pipeline', 'memory', 'idx', 'X_test', 'X_train', 'exp_id', 'log_plots_param', 'y_test', 'y_train', 'fix_imbalance', 'fold_shuffle_param', 'gpu_param', 'html_param', 'data', 'logging_param', 'fold_generator', 'y', 'X', 'is_multiclass', '_available_plots', 'gpu_n_jobs_param'}
2024-04-22 10:04:37,869:INFO:Checking environment
2024-04-22 10:04:37,869:INFO:python_version: 3.11.0
2024-04-22 10:04:37,869:INFO:python_build: ('main', 'Oct 24 2022 18:26:48')
2024-04-22 10:04:37,869:INFO:machine: AMD64
2024-04-22 10:04:37,869:INFO:platform: Windows-10-10.0.22631-SP0
2024-04-22 10:04:37,885:INFO:Memory: svmem(total=3946209280, available=367149056, percent=90.7, used=3579060224, free=367149056)
2024-04-22 10:04:37,885:INFO:Physical Core: 2
2024-04-22 10:04:37,885:INFO:Logical Core: 4
2024-04-22 10:04:37,885:INFO:Checking libraries
2024-04-22 10:04:37,885:INFO:System:
2024-04-22 10:04:37,885:INFO:    python: 3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]
2024-04-22 10:04:37,885:INFO:executable: c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\python.exe
2024-04-22 10:04:37,885:INFO:   machine: Windows-10-10.0.22631-SP0
2024-04-22 10:04:37,885:INFO:PyCaret required dependencies:
2024-04-22 10:04:44,839:INFO:                 pip: 24.0
2024-04-22 10:04:44,839:INFO:          setuptools: 68.2.2
2024-04-22 10:04:44,839:INFO:             pycaret: 3.2.0
2024-04-22 10:04:44,839:INFO:             IPython: 8.16.1
2024-04-22 10:04:44,839:INFO:          ipywidgets: 8.1.1
2024-04-22 10:04:44,839:INFO:                tqdm: 4.66.1
2024-04-22 10:04:44,839:INFO:               numpy: 1.25.2
2024-04-22 10:04:44,839:INFO:              pandas: 1.5.3
2024-04-22 10:04:44,839:INFO:              jinja2: 3.1.2
2024-04-22 10:04:44,839:INFO:               scipy: 1.10.1
2024-04-22 10:04:44,839:INFO:              joblib: 1.3.2
2024-04-22 10:04:44,839:INFO:             sklearn: 1.2.2
2024-04-22 10:04:44,839:INFO:                pyod: 1.1.3
2024-04-22 10:04:44,839:INFO:            imblearn: 0.12.0
2024-04-22 10:04:44,839:INFO:   category_encoders: 2.6.3
2024-04-22 10:04:44,839:INFO:            lightgbm: 4.3.0
2024-04-22 10:04:44,839:INFO:               numba: 0.58.1
2024-04-22 10:04:44,839:INFO:            requests: 2.31.0
2024-04-22 10:04:44,839:INFO:          matplotlib: 3.6.0
2024-04-22 10:04:44,839:INFO:          scikitplot: 0.3.7
2024-04-22 10:04:44,839:INFO:         yellowbrick: 1.5
2024-04-22 10:04:44,839:INFO:              plotly: 5.19.0
2024-04-22 10:04:44,839:INFO:    plotly-resampler: Not installed
2024-04-22 10:04:44,839:INFO:             kaleido: 0.2.1
2024-04-22 10:04:44,839:INFO:           schemdraw: 0.15
2024-04-22 10:04:44,839:INFO:         statsmodels: 0.14.0
2024-04-22 10:04:44,839:INFO:              sktime: 0.21.1
2024-04-22 10:04:44,839:INFO:               tbats: 1.1.3
2024-04-22 10:04:44,839:INFO:            pmdarima: 2.0.4
2024-04-22 10:04:44,839:INFO:              psutil: 5.9.6
2024-04-22 10:04:44,839:INFO:          markupsafe: 2.1.3
2024-04-22 10:04:44,839:INFO:             pickle5: Not installed
2024-04-22 10:04:44,839:INFO:         cloudpickle: 2.2.1
2024-04-22 10:04:44,839:INFO:         deprecation: 2.1.0
2024-04-22 10:04:44,839:INFO:              xxhash: 3.4.1
2024-04-22 10:04:44,839:INFO:           wurlitzer: Not installed
2024-04-22 10:04:44,839:INFO:PyCaret optional dependencies:
2024-04-22 10:04:50,330:INFO:                shap: 0.44.1
2024-04-22 10:04:50,330:INFO:           interpret: 0.5.1
2024-04-22 10:04:50,330:INFO:                umap: 0.5.5
2024-04-22 10:04:50,330:INFO:     ydata_profiling: 4.6.0
2024-04-22 10:04:50,330:INFO:  explainerdashboard: 0.4.5
2024-04-22 10:04:50,330:INFO:             autoviz: Not installed
2024-04-22 10:04:50,330:INFO:           fairlearn: 0.7.0
2024-04-22 10:04:50,330:INFO:          deepchecks: Not installed
2024-04-22 10:04:50,330:INFO:             xgboost: Not installed
2024-04-22 10:04:50,330:INFO:            catboost: 1.2.2
2024-04-22 10:04:50,330:INFO:              kmodes: 0.12.2
2024-04-22 10:04:50,330:INFO:             mlxtend: 0.23.1
2024-04-22 10:04:50,330:INFO:       statsforecast: 1.5.0
2024-04-22 10:04:50,330:INFO:        tune_sklearn: Not installed
2024-04-22 10:04:50,330:INFO:                 ray: Not installed
2024-04-22 10:04:50,330:INFO:            hyperopt: 0.2.7
2024-04-22 10:04:50,330:INFO:              optuna: 3.5.0
2024-04-22 10:04:50,330:INFO:               skopt: 0.9.0
2024-04-22 10:04:50,330:INFO:              mlflow: 1.30.1
2024-04-22 10:04:50,330:INFO:              gradio: 3.50.2
2024-04-22 10:04:50,330:INFO:             fastapi: 0.109.2
2024-04-22 10:04:50,330:INFO:             uvicorn: 0.27.1
2024-04-22 10:04:50,330:INFO:              m2cgen: 0.10.0
2024-04-22 10:04:50,330:INFO:           evidently: 0.2.8
2024-04-22 10:04:50,330:INFO:               fugue: 0.8.6
2024-04-22 10:04:50,330:INFO:           streamlit: 1.33.0
2024-04-22 10:04:50,330:INFO:             prophet: Not installed
2024-04-22 10:04:50,330:INFO:None
2024-04-22 10:04:50,330:INFO:Set up data.
2024-04-22 10:04:50,411:INFO:Set up folding strategy.
2024-04-22 10:04:50,411:INFO:Set up train/test split.
2024-04-22 10:04:50,440:INFO:Set up index.
2024-04-22 10:04:50,440:INFO:Assigning column types.
2024-04-22 10:04:50,449:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-04-22 10:04:50,502:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-04-22 10:04:50,512:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-22 10:04:50,569:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:04:50,569:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:04:52,713:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-04-22 10:04:52,713:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-22 10:04:52,754:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:04:52,754:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:04:52,754:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-04-22 10:04:52,809:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-22 10:04:52,849:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:04:52,849:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:04:52,889:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-22 10:04:53,020:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:04:53,020:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:04:53,029:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-04-22 10:04:53,100:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:04:53,100:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:04:53,189:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:04:53,189:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:04:53,813:INFO:Preparing preprocessing pipeline...
2024-04-22 10:04:53,819:INFO:Set up simple imputation.
2024-04-22 10:04:53,829:INFO:Set up column name cleaning.
2024-04-22 10:04:53,911:INFO:Finished creating preprocessing pipeline.
2024-04-22 10:04:53,924:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\BAGAS\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Number of times pregnant',
                                             'Plasma glucose concentration a 2 '
                                             'hours in an oral glucose '
                                             'tolerance test',
                                             'Diastolic blood pressure (mm Hg)',
                                             'Triceps skin fold thickness (mm)',
                                             '2-Hour serum insulin (mu U/ml)',
                                             'Body mass index (weigh...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-04-22 10:04:53,924:INFO:Creating final display dataframe.
2024-04-22 10:04:54,005:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target    Class variable
2                   Target type            Binary
3           Original data shape          (768, 9)
4        Transformed data shape          (768, 9)
5   Transformed train set shape          (537, 9)
6    Transformed test set shape          (231, 9)
7              Numeric features                 8
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              0771
2024-04-22 10:04:54,151:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:04:54,151:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:04:54,343:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:04:54,343:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:04:54,350:INFO:setup() successfully completed in 16.49s...............
2024-04-22 10:04:54,355:INFO:Initializing compare_models()
2024-04-22 10:04:54,355:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-04-22 10:04:54,355:INFO:Checking exceptions
2024-04-22 10:04:54,360:INFO:Preparing display monitor
2024-04-22 10:04:54,403:INFO:Initializing Logistic Regression
2024-04-22 10:04:54,404:INFO:Total runtime is 2.5125344594319663e-05 minutes
2024-04-22 10:04:54,404:INFO:SubProcess create_model() called ==================================
2024-04-22 10:04:54,416:INFO:Initializing create_model()
2024-04-22 10:04:54,416:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:04:54,416:INFO:Checking exceptions
2024-04-22 10:04:54,416:INFO:Importing libraries
2024-04-22 10:04:54,416:INFO:Copying training dataset
2024-04-22 10:04:54,420:INFO:Defining folds
2024-04-22 10:04:54,420:INFO:Declaring metric variables
2024-04-22 10:04:54,432:INFO:Importing untrained model
2024-04-22 10:04:54,452:INFO:Logistic Regression Imported successfully
2024-04-22 10:04:54,504:INFO:Starting cross validation
2024-04-22 10:04:54,514:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:05:15,731:INFO:Calculating mean and std
2024-04-22 10:05:15,888:INFO:Creating metrics dataframe
2024-04-22 10:05:16,291:INFO:Uploading results into container
2024-04-22 10:05:16,320:INFO:Uploading model into container now
2024-04-22 10:05:16,359:INFO:_master_model_container: 1
2024-04-22 10:05:16,359:INFO:_display_container: 2
2024-04-22 10:05:16,379:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-04-22 10:05:16,380:INFO:create_model() successfully completed......................................
2024-04-22 10:05:30,806:INFO:SubProcess create_model() end ==================================
2024-04-22 10:05:30,809:INFO:Creating metrics dataframe
2024-04-22 10:05:30,884:INFO:Initializing K Neighbors Classifier
2024-04-22 10:05:30,885:INFO:Total runtime is 0.6080418427785238 minutes
2024-04-22 10:05:30,889:INFO:SubProcess create_model() called ==================================
2024-04-22 10:05:30,889:INFO:Initializing create_model()
2024-04-22 10:05:30,889:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:05:30,889:INFO:Checking exceptions
2024-04-22 10:05:30,889:INFO:Importing libraries
2024-04-22 10:05:30,889:INFO:Copying training dataset
2024-04-22 10:05:30,905:INFO:Defining folds
2024-04-22 10:05:30,905:INFO:Declaring metric variables
2024-04-22 10:05:30,909:INFO:Importing untrained model
2024-04-22 10:05:30,914:INFO:K Neighbors Classifier Imported successfully
2024-04-22 10:05:30,924:INFO:Starting cross validation
2024-04-22 10:05:30,927:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:05:32,392:INFO:Calculating mean and std
2024-04-22 10:05:32,397:INFO:Creating metrics dataframe
2024-04-22 10:05:32,420:INFO:Uploading results into container
2024-04-22 10:05:32,420:INFO:Uploading model into container now
2024-04-22 10:05:32,421:INFO:_master_model_container: 2
2024-04-22 10:05:32,421:INFO:_display_container: 2
2024-04-22 10:05:32,421:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-04-22 10:05:32,421:INFO:create_model() successfully completed......................................
2024-04-22 10:05:38,439:INFO:SubProcess create_model() end ==================================
2024-04-22 10:05:38,439:INFO:Creating metrics dataframe
2024-04-22 10:05:38,479:INFO:Initializing Naive Bayes
2024-04-22 10:05:38,479:INFO:Total runtime is 0.7346067110697428 minutes
2024-04-22 10:05:38,484:INFO:SubProcess create_model() called ==================================
2024-04-22 10:05:38,484:INFO:Initializing create_model()
2024-04-22 10:05:38,488:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:05:38,488:INFO:Checking exceptions
2024-04-22 10:05:38,488:INFO:Importing libraries
2024-04-22 10:05:38,490:INFO:Copying training dataset
2024-04-22 10:05:38,490:INFO:Defining folds
2024-04-22 10:05:38,490:INFO:Declaring metric variables
2024-04-22 10:05:38,501:INFO:Importing untrained model
2024-04-22 10:05:38,509:INFO:Naive Bayes Imported successfully
2024-04-22 10:05:38,532:INFO:Starting cross validation
2024-04-22 10:05:38,534:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:05:38,817:INFO:Calculating mean and std
2024-04-22 10:05:38,819:INFO:Creating metrics dataframe
2024-04-22 10:05:38,831:INFO:Uploading results into container
2024-04-22 10:05:38,832:INFO:Uploading model into container now
2024-04-22 10:05:38,833:INFO:_master_model_container: 3
2024-04-22 10:05:38,834:INFO:_display_container: 2
2024-04-22 10:05:38,838:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-04-22 10:05:38,838:INFO:create_model() successfully completed......................................
2024-04-22 10:05:39,131:INFO:SubProcess create_model() end ==================================
2024-04-22 10:05:39,131:INFO:Creating metrics dataframe
2024-04-22 10:05:39,139:INFO:Initializing Decision Tree Classifier
2024-04-22 10:05:39,139:INFO:Total runtime is 0.7456093072891236 minutes
2024-04-22 10:05:39,139:INFO:SubProcess create_model() called ==================================
2024-04-22 10:05:39,139:INFO:Initializing create_model()
2024-04-22 10:05:39,139:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:05:39,139:INFO:Checking exceptions
2024-04-22 10:05:39,139:INFO:Importing libraries
2024-04-22 10:05:39,139:INFO:Copying training dataset
2024-04-22 10:05:39,151:INFO:Defining folds
2024-04-22 10:05:39,152:INFO:Declaring metric variables
2024-04-22 10:05:39,164:INFO:Importing untrained model
2024-04-22 10:05:39,169:INFO:Decision Tree Classifier Imported successfully
2024-04-22 10:05:39,188:INFO:Starting cross validation
2024-04-22 10:05:39,189:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:05:39,414:INFO:Calculating mean and std
2024-04-22 10:05:39,415:INFO:Creating metrics dataframe
2024-04-22 10:05:39,426:INFO:Uploading results into container
2024-04-22 10:05:39,426:INFO:Uploading model into container now
2024-04-22 10:05:39,426:INFO:_master_model_container: 4
2024-04-22 10:05:39,426:INFO:_display_container: 2
2024-04-22 10:05:39,426:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=123, splitter='best')
2024-04-22 10:05:39,426:INFO:create_model() successfully completed......................................
2024-04-22 10:05:39,786:INFO:SubProcess create_model() end ==================================
2024-04-22 10:05:39,786:INFO:Creating metrics dataframe
2024-04-22 10:05:39,801:INFO:Initializing SVM - Linear Kernel
2024-04-22 10:05:39,801:INFO:Total runtime is 0.7566343704859416 minutes
2024-04-22 10:05:39,809:INFO:SubProcess create_model() called ==================================
2024-04-22 10:05:39,809:INFO:Initializing create_model()
2024-04-22 10:05:39,809:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:05:39,809:INFO:Checking exceptions
2024-04-22 10:05:39,809:INFO:Importing libraries
2024-04-22 10:05:39,809:INFO:Copying training dataset
2024-04-22 10:05:39,815:INFO:Defining folds
2024-04-22 10:05:39,815:INFO:Declaring metric variables
2024-04-22 10:05:39,826:INFO:Importing untrained model
2024-04-22 10:05:39,831:INFO:SVM - Linear Kernel Imported successfully
2024-04-22 10:05:39,841:INFO:Starting cross validation
2024-04-22 10:05:39,841:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:05:40,323:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:05:40,329:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:05:40,329:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:05:40,329:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:05:40,402:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:05:40,414:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:05:40,414:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:05:40,419:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:05:40,446:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:05:40,465:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:05:40,466:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:05:40,472:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:05:40,487:INFO:Calculating mean and std
2024-04-22 10:05:40,489:INFO:Creating metrics dataframe
2024-04-22 10:05:40,495:INFO:Uploading results into container
2024-04-22 10:05:40,495:INFO:Uploading model into container now
2024-04-22 10:05:40,495:INFO:_master_model_container: 5
2024-04-22 10:05:40,495:INFO:_display_container: 2
2024-04-22 10:05:40,495:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-04-22 10:05:40,500:INFO:create_model() successfully completed......................................
2024-04-22 10:05:40,784:INFO:SubProcess create_model() end ==================================
2024-04-22 10:05:40,790:INFO:Creating metrics dataframe
2024-04-22 10:05:40,801:INFO:Initializing Ridge Classifier
2024-04-22 10:05:40,801:INFO:Total runtime is 0.7732983191808065 minutes
2024-04-22 10:05:40,808:INFO:SubProcess create_model() called ==================================
2024-04-22 10:05:40,808:INFO:Initializing create_model()
2024-04-22 10:05:40,808:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:05:40,808:INFO:Checking exceptions
2024-04-22 10:05:40,808:INFO:Importing libraries
2024-04-22 10:05:40,809:INFO:Copying training dataset
2024-04-22 10:05:40,811:INFO:Defining folds
2024-04-22 10:05:40,811:INFO:Declaring metric variables
2024-04-22 10:05:40,819:INFO:Importing untrained model
2024-04-22 10:05:40,825:INFO:Ridge Classifier Imported successfully
2024-04-22 10:05:40,837:INFO:Starting cross validation
2024-04-22 10:05:40,837:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:05:40,909:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:05:40,909:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:05:40,930:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:05:40,935:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:05:40,945:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:05:40,945:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:05:40,975:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:05:40,981:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:05:40,982:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:05:40,985:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:05:41,003:INFO:Calculating mean and std
2024-04-22 10:05:41,003:INFO:Creating metrics dataframe
2024-04-22 10:05:41,003:INFO:Uploading results into container
2024-04-22 10:05:41,003:INFO:Uploading model into container now
2024-04-22 10:05:41,003:INFO:_master_model_container: 6
2024-04-22 10:05:41,003:INFO:_display_container: 2
2024-04-22 10:05:41,003:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-04-22 10:05:41,008:INFO:create_model() successfully completed......................................
2024-04-22 10:05:41,278:INFO:SubProcess create_model() end ==================================
2024-04-22 10:05:41,278:INFO:Creating metrics dataframe
2024-04-22 10:05:41,289:INFO:Initializing Random Forest Classifier
2024-04-22 10:05:41,289:INFO:Total runtime is 0.7814380089441936 minutes
2024-04-22 10:05:41,294:INFO:SubProcess create_model() called ==================================
2024-04-22 10:05:41,296:INFO:Initializing create_model()
2024-04-22 10:05:41,296:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:05:41,296:INFO:Checking exceptions
2024-04-22 10:05:41,296:INFO:Importing libraries
2024-04-22 10:05:41,296:INFO:Copying training dataset
2024-04-22 10:05:41,302:INFO:Defining folds
2024-04-22 10:05:41,302:INFO:Declaring metric variables
2024-04-22 10:05:41,310:INFO:Importing untrained model
2024-04-22 10:05:41,310:INFO:Random Forest Classifier Imported successfully
2024-04-22 10:05:41,333:INFO:Starting cross validation
2024-04-22 10:05:41,338:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:05:43,156:INFO:Calculating mean and std
2024-04-22 10:05:43,160:INFO:Creating metrics dataframe
2024-04-22 10:05:43,210:INFO:Uploading results into container
2024-04-22 10:05:43,210:INFO:Uploading model into container now
2024-04-22 10:05:43,210:INFO:_master_model_container: 7
2024-04-22 10:05:43,210:INFO:_display_container: 2
2024-04-22 10:05:43,210:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=123, verbose=0, warm_start=False)
2024-04-22 10:05:43,266:INFO:create_model() successfully completed......................................
2024-04-22 10:05:43,844:INFO:SubProcess create_model() end ==================================
2024-04-22 10:05:43,844:INFO:Creating metrics dataframe
2024-04-22 10:05:43,968:INFO:Initializing Quadratic Discriminant Analysis
2024-04-22 10:05:43,968:INFO:Total runtime is 0.8260909835497539 minutes
2024-04-22 10:05:43,971:INFO:SubProcess create_model() called ==================================
2024-04-22 10:05:43,975:INFO:Initializing create_model()
2024-04-22 10:05:43,975:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:05:43,975:INFO:Checking exceptions
2024-04-22 10:05:43,975:INFO:Importing libraries
2024-04-22 10:05:43,975:INFO:Copying training dataset
2024-04-22 10:05:43,979:INFO:Defining folds
2024-04-22 10:05:43,979:INFO:Declaring metric variables
2024-04-22 10:05:43,985:INFO:Importing untrained model
2024-04-22 10:05:43,990:INFO:Quadratic Discriminant Analysis Imported successfully
2024-04-22 10:05:44,000:INFO:Starting cross validation
2024-04-22 10:05:44,000:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:05:44,269:INFO:Calculating mean and std
2024-04-22 10:05:44,271:INFO:Creating metrics dataframe
2024-04-22 10:05:44,276:INFO:Uploading results into container
2024-04-22 10:05:44,276:INFO:Uploading model into container now
2024-04-22 10:05:44,279:INFO:_master_model_container: 8
2024-04-22 10:05:44,279:INFO:_display_container: 2
2024-04-22 10:05:44,280:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-04-22 10:05:44,280:INFO:create_model() successfully completed......................................
2024-04-22 10:05:44,647:INFO:SubProcess create_model() end ==================================
2024-04-22 10:05:44,649:INFO:Creating metrics dataframe
2024-04-22 10:05:44,670:INFO:Initializing Ada Boost Classifier
2024-04-22 10:05:44,670:INFO:Total runtime is 0.837776796023051 minutes
2024-04-22 10:05:44,679:INFO:SubProcess create_model() called ==================================
2024-04-22 10:05:44,679:INFO:Initializing create_model()
2024-04-22 10:05:44,679:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:05:44,679:INFO:Checking exceptions
2024-04-22 10:05:44,679:INFO:Importing libraries
2024-04-22 10:05:44,679:INFO:Copying training dataset
2024-04-22 10:05:44,700:INFO:Defining folds
2024-04-22 10:05:44,700:INFO:Declaring metric variables
2024-04-22 10:05:44,703:INFO:Importing untrained model
2024-04-22 10:05:44,719:INFO:Ada Boost Classifier Imported successfully
2024-04-22 10:05:44,751:INFO:Starting cross validation
2024-04-22 10:05:44,751:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:05:45,551:INFO:Calculating mean and std
2024-04-22 10:05:45,551:INFO:Creating metrics dataframe
2024-04-22 10:05:45,551:INFO:Uploading results into container
2024-04-22 10:05:45,551:INFO:Uploading model into container now
2024-04-22 10:05:45,551:INFO:_master_model_container: 9
2024-04-22 10:05:45,551:INFO:_display_container: 2
2024-04-22 10:05:45,551:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=123)
2024-04-22 10:05:45,551:INFO:create_model() successfully completed......................................
2024-04-22 10:05:45,941:INFO:SubProcess create_model() end ==================================
2024-04-22 10:05:45,941:INFO:Creating metrics dataframe
2024-04-22 10:05:45,982:INFO:Initializing Gradient Boosting Classifier
2024-04-22 10:05:45,982:INFO:Total runtime is 0.8596595287322999 minutes
2024-04-22 10:05:46,009:INFO:SubProcess create_model() called ==================================
2024-04-22 10:05:46,009:INFO:Initializing create_model()
2024-04-22 10:05:46,009:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:05:46,009:INFO:Checking exceptions
2024-04-22 10:05:46,010:INFO:Importing libraries
2024-04-22 10:05:46,010:INFO:Copying training dataset
2024-04-22 10:05:46,017:INFO:Defining folds
2024-04-22 10:05:46,018:INFO:Declaring metric variables
2024-04-22 10:05:46,022:INFO:Importing untrained model
2024-04-22 10:05:46,022:INFO:Gradient Boosting Classifier Imported successfully
2024-04-22 10:05:46,050:INFO:Starting cross validation
2024-04-22 10:05:46,050:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:05:47,836:INFO:Calculating mean and std
2024-04-22 10:05:47,839:INFO:Creating metrics dataframe
2024-04-22 10:05:47,852:INFO:Uploading results into container
2024-04-22 10:05:47,854:INFO:Uploading model into container now
2024-04-22 10:05:47,856:INFO:_master_model_container: 10
2024-04-22 10:05:47,856:INFO:_display_container: 2
2024-04-22 10:05:47,857:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-04-22 10:05:47,857:INFO:create_model() successfully completed......................................
2024-04-22 10:05:48,161:INFO:SubProcess create_model() end ==================================
2024-04-22 10:05:48,161:INFO:Creating metrics dataframe
2024-04-22 10:05:48,179:INFO:Initializing Linear Discriminant Analysis
2024-04-22 10:05:48,179:INFO:Total runtime is 0.8962704181671144 minutes
2024-04-22 10:05:48,182:INFO:SubProcess create_model() called ==================================
2024-04-22 10:05:48,185:INFO:Initializing create_model()
2024-04-22 10:05:48,185:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:05:48,185:INFO:Checking exceptions
2024-04-22 10:05:48,185:INFO:Importing libraries
2024-04-22 10:05:48,185:INFO:Copying training dataset
2024-04-22 10:05:48,193:INFO:Defining folds
2024-04-22 10:05:48,193:INFO:Declaring metric variables
2024-04-22 10:05:48,201:INFO:Importing untrained model
2024-04-22 10:05:48,203:INFO:Linear Discriminant Analysis Imported successfully
2024-04-22 10:05:48,209:INFO:Starting cross validation
2024-04-22 10:05:48,209:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:05:48,421:INFO:Calculating mean and std
2024-04-22 10:05:48,425:INFO:Creating metrics dataframe
2024-04-22 10:05:48,435:INFO:Uploading results into container
2024-04-22 10:05:48,437:INFO:Uploading model into container now
2024-04-22 10:05:48,438:INFO:_master_model_container: 11
2024-04-22 10:05:48,438:INFO:_display_container: 2
2024-04-22 10:05:48,438:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-04-22 10:05:48,438:INFO:create_model() successfully completed......................................
2024-04-22 10:05:48,739:INFO:SubProcess create_model() end ==================================
2024-04-22 10:05:48,739:INFO:Creating metrics dataframe
2024-04-22 10:05:48,760:INFO:Initializing Extra Trees Classifier
2024-04-22 10:05:48,760:INFO:Total runtime is 0.9059521516164145 minutes
2024-04-22 10:05:48,762:INFO:SubProcess create_model() called ==================================
2024-04-22 10:05:48,766:INFO:Initializing create_model()
2024-04-22 10:05:48,766:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:05:48,766:INFO:Checking exceptions
2024-04-22 10:05:48,766:INFO:Importing libraries
2024-04-22 10:05:48,766:INFO:Copying training dataset
2024-04-22 10:05:48,769:INFO:Defining folds
2024-04-22 10:05:48,769:INFO:Declaring metric variables
2024-04-22 10:05:48,777:INFO:Importing untrained model
2024-04-22 10:05:48,783:INFO:Extra Trees Classifier Imported successfully
2024-04-22 10:05:48,785:INFO:Starting cross validation
2024-04-22 10:05:48,790:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:05:50,397:INFO:Calculating mean and std
2024-04-22 10:05:50,399:INFO:Creating metrics dataframe
2024-04-22 10:05:50,401:INFO:Uploading results into container
2024-04-22 10:05:50,401:INFO:Uploading model into container now
2024-04-22 10:05:50,401:INFO:_master_model_container: 12
2024-04-22 10:05:50,401:INFO:_display_container: 2
2024-04-22 10:05:50,401:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=123, verbose=0, warm_start=False)
2024-04-22 10:05:50,401:INFO:create_model() successfully completed......................................
2024-04-22 10:05:50,694:INFO:SubProcess create_model() end ==================================
2024-04-22 10:05:50,694:INFO:Creating metrics dataframe
2024-04-22 10:05:50,939:INFO:Initializing Light Gradient Boosting Machine
2024-04-22 10:05:50,939:INFO:Total runtime is 0.9422730803489686 minutes
2024-04-22 10:05:50,945:INFO:SubProcess create_model() called ==================================
2024-04-22 10:05:50,945:INFO:Initializing create_model()
2024-04-22 10:05:50,945:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:05:50,946:INFO:Checking exceptions
2024-04-22 10:05:50,946:INFO:Importing libraries
2024-04-22 10:05:50,946:INFO:Copying training dataset
2024-04-22 10:05:50,954:INFO:Defining folds
2024-04-22 10:05:50,954:INFO:Declaring metric variables
2024-04-22 10:05:50,959:INFO:Importing untrained model
2024-04-22 10:05:50,966:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:05:50,974:INFO:Starting cross validation
2024-04-22 10:05:50,974:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:05:52,298:INFO:Calculating mean and std
2024-04-22 10:05:52,299:INFO:Creating metrics dataframe
2024-04-22 10:05:52,305:INFO:Uploading results into container
2024-04-22 10:05:52,305:INFO:Uploading model into container now
2024-04-22 10:05:52,305:INFO:_master_model_container: 13
2024-04-22 10:05:52,309:INFO:_display_container: 2
2024-04-22 10:05:52,309:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:05:52,309:INFO:create_model() successfully completed......................................
2024-04-22 10:05:52,893:INFO:SubProcess create_model() end ==================================
2024-04-22 10:05:52,893:INFO:Creating metrics dataframe
2024-04-22 10:05:52,925:INFO:Initializing CatBoost Classifier
2024-04-22 10:05:52,930:INFO:Total runtime is 0.9754439473152162 minutes
2024-04-22 10:05:52,940:INFO:SubProcess create_model() called ==================================
2024-04-22 10:05:52,941:INFO:Initializing create_model()
2024-04-22 10:05:52,941:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:05:52,941:INFO:Checking exceptions
2024-04-22 10:05:52,941:INFO:Importing libraries
2024-04-22 10:05:52,941:INFO:Copying training dataset
2024-04-22 10:05:52,956:INFO:Defining folds
2024-04-22 10:05:52,956:INFO:Declaring metric variables
2024-04-22 10:05:52,973:INFO:Importing untrained model
2024-04-22 10:05:53,006:INFO:CatBoost Classifier Imported successfully
2024-04-22 10:05:53,050:INFO:Starting cross validation
2024-04-22 10:05:53,050:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:06:09,666:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\model_selection\_validation.py:378: FitFailedWarning: 
3 fits failed out of a total of 10.
The score on these train-test partitions for these parameters will be set to 0.0.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
--------------------------------------------------------------------------------
3 fits failed with the following error:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\model_selection\_validation.py", line 686, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 275, in fit
    fitted_estimator = self._memory_fit(
                       ^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\joblib\memory.py", line 353, in __call__
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 68, in _fit_one
    transformer.fit(*args, **fit_params)
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\catboost\core.py", line 5100, in fit
    self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline, use_best_model,
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\catboost\core.py", line 2319, in _fit
    self._train(
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\catboost\core.py", line 1723, in _train
    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)
  File "_catboost.pyx", line 4645, in _catboost._CatBoost._train
  File "_catboost.pyx", line 4694, in _catboost._CatBoost._train
_catboost.CatBoostError: C:/Go_Agent/pipelines/BuildMaster/catboost.git/catboost/libs/train_lib/dir_helper.cpp:20: Can't create train working dir: catboost_info

  warnings.warn(some_fits_failed_message, FitFailedWarning)

2024-04-22 10:06:09,692:INFO:Calculating mean and std
2024-04-22 10:06:09,753:INFO:Creating metrics dataframe
2024-04-22 10:06:09,868:INFO:Uploading results into container
2024-04-22 10:06:09,883:INFO:Uploading model into container now
2024-04-22 10:06:09,896:INFO:_master_model_container: 14
2024-04-22 10:06:09,896:INFO:_display_container: 2
2024-04-22 10:06:09,898:INFO:<catboost.core.CatBoostClassifier object at 0x0000019A310E9590>
2024-04-22 10:06:09,900:INFO:create_model() successfully completed......................................
2024-04-22 10:06:14,371:WARNING:create_model() for <catboost.core.CatBoostClassifier object at 0x0000019A310E9590> raised an exception or returned all 0.0, trying without fit_kwargs:
2024-04-22 10:06:14,449:WARNING:Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 796, in compare_models
    assert (
AssertionError

2024-04-22 10:06:14,449:INFO:Initializing create_model()
2024-04-22 10:06:14,449:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:06:14,449:INFO:Checking exceptions
2024-04-22 10:06:14,449:INFO:Importing libraries
2024-04-22 10:06:14,449:INFO:Copying training dataset
2024-04-22 10:06:14,480:INFO:Defining folds
2024-04-22 10:06:14,480:INFO:Declaring metric variables
2024-04-22 10:06:14,490:INFO:Importing untrained model
2024-04-22 10:06:14,511:INFO:CatBoost Classifier Imported successfully
2024-04-22 10:06:14,524:INFO:Starting cross validation
2024-04-22 10:06:14,527:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:06:33,300:INFO:Calculating mean and std
2024-04-22 10:06:33,343:INFO:Creating metrics dataframe
2024-04-22 10:06:33,434:INFO:Uploading results into container
2024-04-22 10:06:33,443:INFO:Uploading model into container now
2024-04-22 10:06:33,456:INFO:_master_model_container: 15
2024-04-22 10:06:33,456:INFO:_display_container: 2
2024-04-22 10:06:33,456:INFO:<catboost.core.CatBoostClassifier object at 0x0000019A19831D90>
2024-04-22 10:06:33,456:INFO:create_model() successfully completed......................................
2024-04-22 10:06:39,589:INFO:SubProcess create_model() end ==================================
2024-04-22 10:06:39,589:INFO:Creating metrics dataframe
2024-04-22 10:06:41,486:INFO:Initializing Dummy Classifier
2024-04-22 10:06:41,486:INFO:Total runtime is 1.7847173372904461 minutes
2024-04-22 10:06:41,493:INFO:SubProcess create_model() called ==================================
2024-04-22 10:06:41,493:INFO:Initializing create_model()
2024-04-22 10:06:41,493:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A2F5513D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:06:41,500:INFO:Checking exceptions
2024-04-22 10:06:41,500:INFO:Importing libraries
2024-04-22 10:06:41,509:INFO:Copying training dataset
2024-04-22 10:06:41,605:INFO:Defining folds
2024-04-22 10:06:41,609:INFO:Declaring metric variables
2024-04-22 10:06:41,614:INFO:Importing untrained model
2024-04-22 10:06:41,619:INFO:Dummy Classifier Imported successfully
2024-04-22 10:06:41,633:INFO:Starting cross validation
2024-04-22 10:06:41,636:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:06:41,912:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:06:41,912:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:06:41,912:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:06:41,912:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:06:41,997:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:06:42,010:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:06:42,014:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:06:42,034:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:06:42,034:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:06:42,044:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:06:42,059:INFO:Calculating mean and std
2024-04-22 10:06:42,059:INFO:Creating metrics dataframe
2024-04-22 10:06:42,069:INFO:Uploading results into container
2024-04-22 10:06:42,069:INFO:Uploading model into container now
2024-04-22 10:06:42,069:INFO:_master_model_container: 16
2024-04-22 10:06:42,069:INFO:_display_container: 2
2024-04-22 10:06:42,071:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-04-22 10:06:42,071:INFO:create_model() successfully completed......................................
2024-04-22 10:06:43,119:INFO:SubProcess create_model() end ==================================
2024-04-22 10:06:43,119:INFO:Creating metrics dataframe
2024-04-22 10:06:43,160:INFO:Initializing create_model()
2024-04-22 10:06:43,160:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:06:43,160:INFO:Checking exceptions
2024-04-22 10:06:43,160:INFO:Importing libraries
2024-04-22 10:06:43,160:INFO:Copying training dataset
2024-04-22 10:06:43,170:INFO:Defining folds
2024-04-22 10:06:43,170:INFO:Declaring metric variables
2024-04-22 10:06:43,170:INFO:Importing untrained model
2024-04-22 10:06:43,170:INFO:Declaring custom model
2024-04-22 10:06:43,173:INFO:Logistic Regression Imported successfully
2024-04-22 10:06:43,175:INFO:Cross validation set to False
2024-04-22 10:06:43,175:INFO:Fitting Model
2024-04-22 10:06:43,229:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-04-22 10:06:43,229:INFO:create_model() successfully completed......................................
2024-04-22 10:06:45,166:INFO:_master_model_container: 16
2024-04-22 10:06:45,242:INFO:_display_container: 2
2024-04-22 10:06:45,246:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-04-22 10:06:45,246:INFO:compare_models() successfully completed......................................
2024-04-22 10:06:45,874:INFO:Initializing tune_model()
2024-04-22 10:06:45,877:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-04-22 10:06:45,877:INFO:Checking exceptions
2024-04-22 10:06:46,149:INFO:Copying training dataset
2024-04-22 10:06:46,153:INFO:Checking base model
2024-04-22 10:06:46,154:INFO:Base model : Logistic Regression
2024-04-22 10:06:46,163:INFO:Declaring metric variables
2024-04-22 10:06:46,168:INFO:Defining Hyperparameters
2024-04-22 10:06:46,765:INFO:Tuning with n_jobs=-1
2024-04-22 10:06:46,765:INFO:Initializing RandomizedSearchCV
2024-04-22 10:06:50,101:INFO:best_params: {'actual_estimator__class_weight': {}, 'actual_estimator__C': 7.863}
2024-04-22 10:06:50,105:INFO:Hyperparameter search completed
2024-04-22 10:06:50,105:INFO:SubProcess create_model() called ==================================
2024-04-22 10:06:50,105:INFO:Initializing create_model()
2024-04-22 10:06:50,105:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000019A31117090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'class_weight': {}, 'C': 7.863})
2024-04-22 10:06:50,105:INFO:Checking exceptions
2024-04-22 10:06:50,105:INFO:Importing libraries
2024-04-22 10:06:50,105:INFO:Copying training dataset
2024-04-22 10:06:50,113:INFO:Defining folds
2024-04-22 10:06:50,113:INFO:Declaring metric variables
2024-04-22 10:06:50,119:INFO:Importing untrained model
2024-04-22 10:06:50,119:INFO:Declaring custom model
2024-04-22 10:06:50,130:INFO:Logistic Regression Imported successfully
2024-04-22 10:06:50,142:INFO:Starting cross validation
2024-04-22 10:06:50,145:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:06:50,840:INFO:Calculating mean and std
2024-04-22 10:06:50,845:INFO:Creating metrics dataframe
2024-04-22 10:06:50,941:INFO:Finalizing model
2024-04-22 10:06:51,329:INFO:Uploading results into container
2024-04-22 10:06:51,329:INFO:Uploading model into container now
2024-04-22 10:06:51,364:INFO:_master_model_container: 17
2024-04-22 10:06:51,365:INFO:_display_container: 3
2024-04-22 10:06:51,365:INFO:LogisticRegression(C=7.863, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-04-22 10:06:51,365:INFO:create_model() successfully completed......................................
2024-04-22 10:06:51,620:INFO:SubProcess create_model() end ==================================
2024-04-22 10:06:51,620:INFO:choose_better activated
2024-04-22 10:06:51,625:INFO:SubProcess create_model() called ==================================
2024-04-22 10:06:51,628:INFO:Initializing create_model()
2024-04-22 10:06:51,628:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:06:51,628:INFO:Checking exceptions
2024-04-22 10:06:51,628:INFO:Importing libraries
2024-04-22 10:06:51,630:INFO:Copying training dataset
2024-04-22 10:06:51,630:INFO:Defining folds
2024-04-22 10:06:51,630:INFO:Declaring metric variables
2024-04-22 10:06:51,630:INFO:Importing untrained model
2024-04-22 10:06:51,630:INFO:Declaring custom model
2024-04-22 10:06:51,630:INFO:Logistic Regression Imported successfully
2024-04-22 10:06:51,630:INFO:Starting cross validation
2024-04-22 10:06:51,630:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:06:51,964:INFO:Calculating mean and std
2024-04-22 10:06:51,965:INFO:Creating metrics dataframe
2024-04-22 10:06:51,968:INFO:Finalizing model
2024-04-22 10:06:52,007:INFO:Uploading results into container
2024-04-22 10:06:52,007:INFO:Uploading model into container now
2024-04-22 10:06:52,007:INFO:_master_model_container: 18
2024-04-22 10:06:52,007:INFO:_display_container: 4
2024-04-22 10:06:52,010:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-04-22 10:06:52,010:INFO:create_model() successfully completed......................................
2024-04-22 10:06:52,309:INFO:SubProcess create_model() end ==================================
2024-04-22 10:06:52,309:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for Accuracy is 0.7689
2024-04-22 10:06:52,309:INFO:LogisticRegression(C=7.863, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for Accuracy is 0.7726
2024-04-22 10:06:52,309:INFO:LogisticRegression(C=7.863, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) is best model
2024-04-22 10:06:52,309:INFO:choose_better completed
2024-04-22 10:06:52,329:INFO:_master_model_container: 18
2024-04-22 10:06:52,329:INFO:_display_container: 3
2024-04-22 10:06:52,329:INFO:LogisticRegression(C=7.863, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-04-22 10:06:52,329:INFO:tune_model() successfully completed......................................
2024-04-22 10:06:52,634:INFO:Initializing evaluate_model()
2024-04-22 10:06:52,634:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=LogisticRegression(C=7.863, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-04-22 10:06:52,672:INFO:Initializing plot_model()
2024-04-22 10:06:52,672:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=LogisticRegression(C=7.863, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-04-22 10:06:52,672:INFO:Checking exceptions
2024-04-22 10:06:52,672:INFO:Preloading libraries
2024-04-22 10:06:52,672:INFO:Copying training dataset
2024-04-22 10:06:52,672:INFO:Plot type: pipeline
2024-04-22 10:06:53,087:INFO:Visual Rendered Successfully
2024-04-22 10:06:54,089:INFO:plot_model() successfully completed......................................
2024-04-22 10:06:54,095:INFO:Initializing finalize_model()
2024-04-22 10:06:54,095:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=LogisticRegression(C=7.863, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-04-22 10:06:54,095:INFO:Finalizing LogisticRegression(C=7.863, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-04-22 10:06:54,103:INFO:Initializing create_model()
2024-04-22 10:06:54,103:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019A1454D450>, estimator=LogisticRegression(C=7.863, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:06:54,103:INFO:Checking exceptions
2024-04-22 10:06:54,109:INFO:Importing libraries
2024-04-22 10:06:54,109:INFO:Copying training dataset
2024-04-22 10:06:54,109:INFO:Defining folds
2024-04-22 10:06:54,109:INFO:Declaring metric variables
2024-04-22 10:06:54,111:INFO:Importing untrained model
2024-04-22 10:06:54,111:INFO:Declaring custom model
2024-04-22 10:06:54,111:INFO:Logistic Regression Imported successfully
2024-04-22 10:06:54,111:INFO:Cross validation set to False
2024-04-22 10:06:54,111:INFO:Fitting Model
2024-04-22 10:06:54,160:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Number of times pregnant',
                                             'Plasma glucose concentration a 2 '
                                             'hours in an oral glucose '
                                             'tolerance test',
                                             'Diastolic blood pressure (mm Hg)',
                                             'Triceps skin fold thickness (mm)',
                                             '2-Hour serum insulin (mu U/ml)',
                                             'Body mass index (weight in '
                                             'kg/(height in m)^2)',
                                             'Diabetes pedigre...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=7.863, class_weight={}, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2024-04-22 10:06:54,160:INFO:create_model() successfully completed......................................
2024-04-22 10:06:54,489:INFO:_master_model_container: 18
2024-04-22 10:06:54,489:INFO:_display_container: 3
2024-04-22 10:06:54,489:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Number of times pregnant',
                                             'Plasma glucose concentration a 2 '
                                             'hours in an oral glucose '
                                             'tolerance test',
                                             'Diastolic blood pressure (mm Hg)',
                                             'Triceps skin fold thickness (mm)',
                                             '2-Hour serum insulin (mu U/ml)',
                                             'Body mass index (weight in '
                                             'kg/(height in m)^2)',
                                             'Diabetes pedigre...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=7.863, class_weight={}, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2024-04-22 10:06:54,489:INFO:finalize_model() successfully completed......................................
2024-04-22 10:06:54,760:INFO:Initializing save_model()
2024-04-22 10:06:54,760:INFO:save_model(model=LogisticRegression(C=7.863, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), model_name=diabetes_pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\BAGAS\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Number of times pregnant',
                                             'Plasma glucose concentration a 2 '
                                             'hours in an oral glucose '
                                             'tolerance test',
                                             'Diastolic blood pressure (mm Hg)',
                                             'Triceps skin fold thickness (mm)',
                                             '2-Hour serum insulin (mu U/ml)',
                                             'Body mass index (weigh...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-04-22 10:06:54,760:INFO:Adding model into prep_pipe
2024-04-22 10:06:54,770:INFO:diabetes_pipeline.pkl saved in current working directory
2024-04-22 10:06:54,780:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Number of times pregnant',
                                             'Plasma glucose concentration a 2 '
                                             'hours in an oral glucose '
                                             'tolerance test',
                                             'Diastolic blood pressure (mm Hg)',
                                             'Triceps skin fold thickness (mm)',
                                             '2-Hour serum insulin (mu U/ml)',
                                             'Body mass index (weight in '
                                             'kg/(height in m)^2)',
                                             'Diabetes pedigre...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LogisticRegression(C=7.863, class_weight={}, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2024-04-22 10:06:54,780:INFO:save_model() successfully completed......................................
2024-04-22 10:16:30,273:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:16:30,273:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:16:30,273:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:16:30,276:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:16:34,729:INFO:Initializing load_model()
2024-04-22 10:16:34,729:INFO:load_model(model_name=diabetes_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 10:19:08,337:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:19:08,338:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:19:08,338:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:19:08,339:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:31:14,525:INFO:Initializing load_model()
2024-04-22 10:31:14,567:INFO:load_model(model_name=diabetes_pipeline, platform=None, authentication=None, verbose=True)
2024-04-22 10:33:01,048:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:33:01,048:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:33:01,048:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:33:01,048:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-04-22 10:33:57,698:INFO:PyCaret ClassificationExperiment
2024-04-22 10:33:57,700:INFO:Logging name: clf-default-name
2024-04-22 10:33:57,700:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-04-22 10:33:57,700:INFO:version 3.2.0
2024-04-22 10:33:57,700:INFO:Initializing setup()
2024-04-22 10:33:57,700:INFO:self.USI: a482
2024-04-22 10:33:57,700:INFO:self._variable_keys: {'log_plots_param', 'X', 'fold_shuffle_param', 'gpu_param', 'html_param', 'seed', '_ml_usecase', 'exp_name_log', 'n_jobs_param', 'data', 'idx', 'y_test', '_available_plots', 'target_param', 'fold_generator', 'fold_groups_param', 'fix_imbalance', 'X_test', 'y', 'USI', 'y_train', 'pipeline', 'X_train', 'logging_param', 'exp_id', 'memory', 'gpu_n_jobs_param', 'is_multiclass'}
2024-04-22 10:33:57,700:INFO:Checking environment
2024-04-22 10:33:57,700:INFO:python_version: 3.11.0
2024-04-22 10:33:57,700:INFO:python_build: ('main', 'Oct 24 2022 18:26:48')
2024-04-22 10:33:57,700:INFO:machine: AMD64
2024-04-22 10:33:57,700:INFO:platform: Windows-10-10.0.22631-SP0
2024-04-22 10:33:57,714:INFO:Memory: svmem(total=3946209280, available=225046528, percent=94.3, used=3721162752, free=225046528)
2024-04-22 10:33:57,714:INFO:Physical Core: 2
2024-04-22 10:33:57,720:INFO:Logical Core: 4
2024-04-22 10:33:57,720:INFO:Checking libraries
2024-04-22 10:33:57,720:INFO:System:
2024-04-22 10:33:57,720:INFO:    python: 3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]
2024-04-22 10:33:57,720:INFO:executable: c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\python.exe
2024-04-22 10:33:57,720:INFO:   machine: Windows-10-10.0.22631-SP0
2024-04-22 10:33:57,720:INFO:PyCaret required dependencies:
2024-04-22 10:34:02,177:INFO:                 pip: 24.0
2024-04-22 10:34:02,177:INFO:          setuptools: 68.2.2
2024-04-22 10:34:02,177:INFO:             pycaret: 3.2.0
2024-04-22 10:34:02,177:INFO:             IPython: 8.16.1
2024-04-22 10:34:02,177:INFO:          ipywidgets: 8.1.1
2024-04-22 10:34:02,177:INFO:                tqdm: 4.66.1
2024-04-22 10:34:02,177:INFO:               numpy: 1.25.2
2024-04-22 10:34:02,177:INFO:              pandas: 1.5.3
2024-04-22 10:34:02,177:INFO:              jinja2: 3.1.2
2024-04-22 10:34:02,177:INFO:               scipy: 1.10.1
2024-04-22 10:34:02,177:INFO:              joblib: 1.3.2
2024-04-22 10:34:02,177:INFO:             sklearn: 1.2.2
2024-04-22 10:34:02,177:INFO:                pyod: 1.1.3
2024-04-22 10:34:02,177:INFO:            imblearn: 0.12.0
2024-04-22 10:34:02,177:INFO:   category_encoders: 2.6.3
2024-04-22 10:34:02,177:INFO:            lightgbm: 4.3.0
2024-04-22 10:34:02,177:INFO:               numba: 0.58.1
2024-04-22 10:34:02,177:INFO:            requests: 2.31.0
2024-04-22 10:34:02,177:INFO:          matplotlib: 3.6.0
2024-04-22 10:34:02,177:INFO:          scikitplot: 0.3.7
2024-04-22 10:34:02,177:INFO:         yellowbrick: 1.5
2024-04-22 10:34:02,177:INFO:              plotly: 5.19.0
2024-04-22 10:34:02,177:INFO:    plotly-resampler: Not installed
2024-04-22 10:34:02,177:INFO:             kaleido: 0.2.1
2024-04-22 10:34:02,177:INFO:           schemdraw: 0.15
2024-04-22 10:34:02,177:INFO:         statsmodels: 0.14.0
2024-04-22 10:34:02,177:INFO:              sktime: 0.21.1
2024-04-22 10:34:02,177:INFO:               tbats: 1.1.3
2024-04-22 10:34:02,177:INFO:            pmdarima: 2.0.4
2024-04-22 10:34:02,177:INFO:              psutil: 5.9.6
2024-04-22 10:34:02,177:INFO:          markupsafe: 2.1.3
2024-04-22 10:34:02,177:INFO:             pickle5: Not installed
2024-04-22 10:34:02,180:INFO:         cloudpickle: 2.2.1
2024-04-22 10:34:02,180:INFO:         deprecation: 2.1.0
2024-04-22 10:34:02,180:INFO:              xxhash: 3.4.1
2024-04-22 10:34:02,180:INFO:           wurlitzer: Not installed
2024-04-22 10:34:02,180:INFO:PyCaret optional dependencies:
2024-04-22 10:34:10,500:INFO:                shap: 0.44.1
2024-04-22 10:34:10,500:INFO:           interpret: 0.5.1
2024-04-22 10:34:10,500:INFO:                umap: 0.5.5
2024-04-22 10:34:10,500:INFO:     ydata_profiling: 4.6.0
2024-04-22 10:34:10,500:INFO:  explainerdashboard: 0.4.5
2024-04-22 10:34:10,500:INFO:             autoviz: Not installed
2024-04-22 10:34:10,500:INFO:           fairlearn: 0.7.0
2024-04-22 10:34:10,500:INFO:          deepchecks: Not installed
2024-04-22 10:34:10,500:INFO:             xgboost: Not installed
2024-04-22 10:34:10,500:INFO:            catboost: 1.2.2
2024-04-22 10:34:10,500:INFO:              kmodes: 0.12.2
2024-04-22 10:34:10,500:INFO:             mlxtend: 0.23.1
2024-04-22 10:34:10,500:INFO:       statsforecast: 1.5.0
2024-04-22 10:34:10,500:INFO:        tune_sklearn: Not installed
2024-04-22 10:34:10,500:INFO:                 ray: Not installed
2024-04-22 10:34:10,500:INFO:            hyperopt: 0.2.7
2024-04-22 10:34:10,500:INFO:              optuna: 3.5.0
2024-04-22 10:34:10,500:INFO:               skopt: 0.9.0
2024-04-22 10:34:10,500:INFO:              mlflow: 1.30.1
2024-04-22 10:34:10,500:INFO:              gradio: 3.50.2
2024-04-22 10:34:10,500:INFO:             fastapi: 0.109.2
2024-04-22 10:34:10,500:INFO:             uvicorn: 0.27.1
2024-04-22 10:34:10,500:INFO:              m2cgen: 0.10.0
2024-04-22 10:34:10,500:INFO:           evidently: 0.2.8
2024-04-22 10:34:10,500:INFO:               fugue: 0.8.6
2024-04-22 10:34:10,500:INFO:           streamlit: 1.33.0
2024-04-22 10:34:10,500:INFO:             prophet: Not installed
2024-04-22 10:34:10,500:INFO:None
2024-04-22 10:34:10,500:INFO:Set up data.
2024-04-22 10:34:10,564:INFO:Set up folding strategy.
2024-04-22 10:34:10,570:INFO:Set up train/test split.
2024-04-22 10:34:10,590:INFO:Set up index.
2024-04-22 10:34:10,590:INFO:Assigning column types.
2024-04-22 10:34:10,604:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-04-22 10:34:10,675:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-04-22 10:34:10,682:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-22 10:34:10,733:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:34:10,736:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:34:10,993:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-04-22 10:34:10,998:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-22 10:34:11,025:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:34:11,025:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:34:11,025:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-04-22 10:34:11,071:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-22 10:34:11,094:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:34:11,094:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:34:11,137:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-04-22 10:34:11,160:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:34:11,160:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:34:11,164:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-04-22 10:34:11,225:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:34:11,225:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:34:11,298:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:34:11,298:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:34:11,300:INFO:Preparing preprocessing pipeline...
2024-04-22 10:34:11,300:INFO:Set up simple imputation.
2024-04-22 10:34:11,300:INFO:Set up column name cleaning.
2024-04-22 10:34:11,440:INFO:Finished creating preprocessing pipeline.
2024-04-22 10:34:11,450:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\BAGAS\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'vents'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strate...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2024-04-22 10:34:11,450:INFO:Creating final display dataframe.
2024-04-22 10:34:11,520:INFO:Setup _display_container:                     Description             Value
0                    Session id                69
1                        Target             happy
2                   Target type            Binary
3           Original data shape          (143, 7)
4        Transformed data shape          (143, 7)
5   Transformed train set shape          (100, 7)
6    Transformed test set shape           (43, 7)
7              Numeric features                 6
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              a482
2024-04-22 10:34:11,620:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:34:11,620:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:34:11,703:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-04-22 10:34:11,703:INFO:Soft dependency imported: catboost: 1.2.2
2024-04-22 10:34:11,703:INFO:setup() successfully completed in 14.17s...............
2024-04-22 10:34:57,340:INFO:Initializing compare_models()
2024-04-22 10:34:57,351:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2024-04-22 10:34:57,351:INFO:Checking exceptions
2024-04-22 10:34:57,575:INFO:Preparing display monitor
2024-04-22 10:34:58,190:INFO:Initializing Logistic Regression
2024-04-22 10:34:58,190:INFO:Total runtime is 0.0 minutes
2024-04-22 10:34:58,190:INFO:SubProcess create_model() called ==================================
2024-04-22 10:34:58,202:INFO:Initializing create_model()
2024-04-22 10:34:58,202:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017733951C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:34:58,205:INFO:Checking exceptions
2024-04-22 10:34:58,206:INFO:Importing libraries
2024-04-22 10:34:58,210:INFO:Copying training dataset
2024-04-22 10:34:58,266:INFO:Defining folds
2024-04-22 10:34:58,270:INFO:Declaring metric variables
2024-04-22 10:34:58,276:INFO:Importing untrained model
2024-04-22 10:34:58,290:INFO:Logistic Regression Imported successfully
2024-04-22 10:34:58,311:INFO:Starting cross validation
2024-04-22 10:34:58,370:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:35:23,160:INFO:Calculating mean and std
2024-04-22 10:35:23,420:INFO:Creating metrics dataframe
2024-04-22 10:35:23,581:INFO:Uploading results into container
2024-04-22 10:35:23,590:INFO:Uploading model into container now
2024-04-22 10:35:23,621:INFO:_master_model_container: 1
2024-04-22 10:35:23,621:INFO:_display_container: 2
2024-04-22 10:35:23,621:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=69, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-04-22 10:35:23,621:INFO:create_model() successfully completed......................................
2024-04-22 10:35:28,321:INFO:SubProcess create_model() end ==================================
2024-04-22 10:35:28,321:INFO:Creating metrics dataframe
2024-04-22 10:35:28,366:INFO:Initializing K Neighbors Classifier
2024-04-22 10:35:28,366:INFO:Total runtime is 0.5029249072074891 minutes
2024-04-22 10:35:28,370:INFO:SubProcess create_model() called ==================================
2024-04-22 10:35:28,370:INFO:Initializing create_model()
2024-04-22 10:35:28,370:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017733951C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:35:28,370:INFO:Checking exceptions
2024-04-22 10:35:28,370:INFO:Importing libraries
2024-04-22 10:35:28,370:INFO:Copying training dataset
2024-04-22 10:35:28,380:INFO:Defining folds
2024-04-22 10:35:28,380:INFO:Declaring metric variables
2024-04-22 10:35:28,380:INFO:Importing untrained model
2024-04-22 10:35:28,390:INFO:K Neighbors Classifier Imported successfully
2024-04-22 10:35:28,395:INFO:Starting cross validation
2024-04-22 10:35:28,395:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:35:28,784:INFO:Calculating mean and std
2024-04-22 10:35:28,784:INFO:Creating metrics dataframe
2024-04-22 10:35:28,793:INFO:Uploading results into container
2024-04-22 10:35:28,793:INFO:Uploading model into container now
2024-04-22 10:35:28,793:INFO:_master_model_container: 2
2024-04-22 10:35:28,793:INFO:_display_container: 2
2024-04-22 10:35:28,793:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-04-22 10:35:28,793:INFO:create_model() successfully completed......................................
2024-04-22 10:35:29,025:INFO:SubProcess create_model() end ==================================
2024-04-22 10:35:29,025:INFO:Creating metrics dataframe
2024-04-22 10:35:29,050:INFO:Initializing Naive Bayes
2024-04-22 10:35:29,050:INFO:Total runtime is 0.5143312613169353 minutes
2024-04-22 10:35:29,060:INFO:SubProcess create_model() called ==================================
2024-04-22 10:35:29,060:INFO:Initializing create_model()
2024-04-22 10:35:29,060:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017733951C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:35:29,060:INFO:Checking exceptions
2024-04-22 10:35:29,060:INFO:Importing libraries
2024-04-22 10:35:29,060:INFO:Copying training dataset
2024-04-22 10:35:29,079:INFO:Defining folds
2024-04-22 10:35:29,079:INFO:Declaring metric variables
2024-04-22 10:35:29,097:INFO:Importing untrained model
2024-04-22 10:35:29,107:INFO:Naive Bayes Imported successfully
2024-04-22 10:35:29,124:INFO:Starting cross validation
2024-04-22 10:35:29,126:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:35:29,343:INFO:Calculating mean and std
2024-04-22 10:35:29,343:INFO:Creating metrics dataframe
2024-04-22 10:35:29,512:INFO:Uploading results into container
2024-04-22 10:35:29,512:INFO:Uploading model into container now
2024-04-22 10:35:29,512:INFO:_master_model_container: 3
2024-04-22 10:35:29,512:INFO:_display_container: 2
2024-04-22 10:35:29,512:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-04-22 10:35:29,512:INFO:create_model() successfully completed......................................
2024-04-22 10:35:29,670:INFO:SubProcess create_model() end ==================================
2024-04-22 10:35:29,670:INFO:Creating metrics dataframe
2024-04-22 10:35:29,685:INFO:Initializing Decision Tree Classifier
2024-04-22 10:35:29,685:INFO:Total runtime is 0.524921480814616 minutes
2024-04-22 10:35:29,690:INFO:SubProcess create_model() called ==================================
2024-04-22 10:35:29,690:INFO:Initializing create_model()
2024-04-22 10:35:29,690:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017733951C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:35:29,690:INFO:Checking exceptions
2024-04-22 10:35:29,690:INFO:Importing libraries
2024-04-22 10:35:29,690:INFO:Copying training dataset
2024-04-22 10:35:29,690:INFO:Defining folds
2024-04-22 10:35:29,690:INFO:Declaring metric variables
2024-04-22 10:35:29,690:INFO:Importing untrained model
2024-04-22 10:35:29,723:INFO:Decision Tree Classifier Imported successfully
2024-04-22 10:35:29,738:INFO:Starting cross validation
2024-04-22 10:35:29,746:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:35:30,265:INFO:Calculating mean and std
2024-04-22 10:35:30,270:INFO:Creating metrics dataframe
2024-04-22 10:35:30,278:INFO:Uploading results into container
2024-04-22 10:35:30,278:INFO:Uploading model into container now
2024-04-22 10:35:30,278:INFO:_master_model_container: 4
2024-04-22 10:35:30,278:INFO:_display_container: 2
2024-04-22 10:35:30,284:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=69, splitter='best')
2024-04-22 10:35:30,284:INFO:create_model() successfully completed......................................
2024-04-22 10:35:30,452:INFO:SubProcess create_model() end ==================================
2024-04-22 10:35:30,452:INFO:Creating metrics dataframe
2024-04-22 10:35:30,572:INFO:Initializing SVM - Linear Kernel
2024-04-22 10:35:30,572:INFO:Total runtime is 0.5396932721138001 minutes
2024-04-22 10:35:30,577:INFO:SubProcess create_model() called ==================================
2024-04-22 10:35:30,577:INFO:Initializing create_model()
2024-04-22 10:35:30,577:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017733951C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:35:30,577:INFO:Checking exceptions
2024-04-22 10:35:30,577:INFO:Importing libraries
2024-04-22 10:35:30,577:INFO:Copying training dataset
2024-04-22 10:35:30,587:INFO:Defining folds
2024-04-22 10:35:30,587:INFO:Declaring metric variables
2024-04-22 10:35:30,624:INFO:Importing untrained model
2024-04-22 10:35:30,632:INFO:SVM - Linear Kernel Imported successfully
2024-04-22 10:35:30,643:INFO:Starting cross validation
2024-04-22 10:35:30,646:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:35:30,720:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:35:30,731:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:35:30,746:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:35:30,758:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:35:30,775:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:35:30,790:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:35:30,845:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:35:30,857:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:35:30,862:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:35:30,880:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2024-04-22 10:35:30,924:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:35:30,945:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1235, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2024-04-22 10:35:30,990:INFO:Calculating mean and std
2024-04-22 10:35:30,996:INFO:Creating metrics dataframe
2024-04-22 10:35:31,012:INFO:Uploading results into container
2024-04-22 10:35:31,012:INFO:Uploading model into container now
2024-04-22 10:35:31,014:INFO:_master_model_container: 5
2024-04-22 10:35:31,014:INFO:_display_container: 2
2024-04-22 10:35:31,014:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=69, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-04-22 10:35:31,014:INFO:create_model() successfully completed......................................
2024-04-22 10:35:31,530:INFO:SubProcess create_model() end ==================================
2024-04-22 10:35:31,532:INFO:Creating metrics dataframe
2024-04-22 10:35:31,560:INFO:Initializing Ridge Classifier
2024-04-22 10:35:31,560:INFO:Total runtime is 0.5561629136403402 minutes
2024-04-22 10:35:31,566:INFO:SubProcess create_model() called ==================================
2024-04-22 10:35:31,569:INFO:Initializing create_model()
2024-04-22 10:35:31,569:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017733951C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:35:31,570:INFO:Checking exceptions
2024-04-22 10:35:31,570:INFO:Importing libraries
2024-04-22 10:35:31,570:INFO:Copying training dataset
2024-04-22 10:35:31,570:INFO:Defining folds
2024-04-22 10:35:31,570:INFO:Declaring metric variables
2024-04-22 10:35:31,589:INFO:Importing untrained model
2024-04-22 10:35:31,595:INFO:Ridge Classifier Imported successfully
2024-04-22 10:35:31,620:INFO:Starting cross validation
2024-04-22 10:35:31,626:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:35:31,710:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:35:31,760:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:35:31,815:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:35:31,815:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:35:31,821:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:35:31,844:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:35:31,856:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:35:31,856:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:35:31,886:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:35:31,888:WARNING:c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\BAGAS\AppData\Local\Programs\Python\Python311\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2024-04-22 10:35:31,929:INFO:Calculating mean and std
2024-04-22 10:35:31,930:INFO:Creating metrics dataframe
2024-04-22 10:35:31,937:INFO:Uploading results into container
2024-04-22 10:35:31,937:INFO:Uploading model into container now
2024-04-22 10:35:31,937:INFO:_master_model_container: 6
2024-04-22 10:35:31,937:INFO:_display_container: 2
2024-04-22 10:35:31,937:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=69, solver='auto',
                tol=0.0001)
2024-04-22 10:35:31,937:INFO:create_model() successfully completed......................................
2024-04-22 10:35:32,111:INFO:SubProcess create_model() end ==================================
2024-04-22 10:35:32,111:INFO:Creating metrics dataframe
2024-04-22 10:35:32,142:INFO:Initializing Random Forest Classifier
2024-04-22 10:35:32,150:INFO:Total runtime is 0.5658660213152568 minutes
2024-04-22 10:35:32,150:INFO:SubProcess create_model() called ==================================
2024-04-22 10:35:32,155:INFO:Initializing create_model()
2024-04-22 10:35:32,155:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017733951C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:35:32,155:INFO:Checking exceptions
2024-04-22 10:35:32,155:INFO:Importing libraries
2024-04-22 10:35:32,155:INFO:Copying training dataset
2024-04-22 10:35:32,158:INFO:Defining folds
2024-04-22 10:35:32,158:INFO:Declaring metric variables
2024-04-22 10:35:32,162:INFO:Importing untrained model
2024-04-22 10:35:32,168:INFO:Random Forest Classifier Imported successfully
2024-04-22 10:35:32,190:INFO:Starting cross validation
2024-04-22 10:35:32,190:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:35:33,660:INFO:Calculating mean and std
2024-04-22 10:35:33,663:INFO:Creating metrics dataframe
2024-04-22 10:35:33,670:INFO:Uploading results into container
2024-04-22 10:35:33,670:INFO:Uploading model into container now
2024-04-22 10:35:33,670:INFO:_master_model_container: 7
2024-04-22 10:35:33,670:INFO:_display_container: 2
2024-04-22 10:35:33,670:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=69, verbose=0, warm_start=False)
2024-04-22 10:35:33,670:INFO:create_model() successfully completed......................................
2024-04-22 10:35:33,870:INFO:SubProcess create_model() end ==================================
2024-04-22 10:35:33,870:INFO:Creating metrics dataframe
2024-04-22 10:35:33,891:INFO:Initializing Quadratic Discriminant Analysis
2024-04-22 10:35:33,891:INFO:Total runtime is 0.5950194398562113 minutes
2024-04-22 10:35:33,913:INFO:SubProcess create_model() called ==================================
2024-04-22 10:35:33,913:INFO:Initializing create_model()
2024-04-22 10:35:33,913:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017733951C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:35:33,913:INFO:Checking exceptions
2024-04-22 10:35:33,913:INFO:Importing libraries
2024-04-22 10:35:33,913:INFO:Copying training dataset
2024-04-22 10:35:33,920:INFO:Defining folds
2024-04-22 10:35:33,920:INFO:Declaring metric variables
2024-04-22 10:35:33,928:INFO:Importing untrained model
2024-04-22 10:35:33,936:INFO:Quadratic Discriminant Analysis Imported successfully
2024-04-22 10:35:33,952:INFO:Starting cross validation
2024-04-22 10:35:33,952:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:35:34,278:INFO:Calculating mean and std
2024-04-22 10:35:34,287:INFO:Creating metrics dataframe
2024-04-22 10:35:34,292:INFO:Uploading results into container
2024-04-22 10:35:34,300:INFO:Uploading model into container now
2024-04-22 10:35:34,300:INFO:_master_model_container: 8
2024-04-22 10:35:34,300:INFO:_display_container: 2
2024-04-22 10:35:34,303:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-04-22 10:35:34,303:INFO:create_model() successfully completed......................................
2024-04-22 10:35:34,479:INFO:SubProcess create_model() end ==================================
2024-04-22 10:35:34,479:INFO:Creating metrics dataframe
2024-04-22 10:35:34,705:INFO:Initializing Ada Boost Classifier
2024-04-22 10:35:34,705:INFO:Total runtime is 0.608588989575704 minutes
2024-04-22 10:35:34,720:INFO:SubProcess create_model() called ==================================
2024-04-22 10:35:34,720:INFO:Initializing create_model()
2024-04-22 10:35:34,720:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017733951C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:35:34,720:INFO:Checking exceptions
2024-04-22 10:35:34,720:INFO:Importing libraries
2024-04-22 10:35:34,720:INFO:Copying training dataset
2024-04-22 10:35:34,720:INFO:Defining folds
2024-04-22 10:35:34,720:INFO:Declaring metric variables
2024-04-22 10:35:34,735:INFO:Importing untrained model
2024-04-22 10:35:34,740:INFO:Ada Boost Classifier Imported successfully
2024-04-22 10:35:34,760:INFO:Starting cross validation
2024-04-22 10:35:34,762:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:35:35,800:INFO:Calculating mean and std
2024-04-22 10:35:35,802:INFO:Creating metrics dataframe
2024-04-22 10:35:35,804:INFO:Uploading results into container
2024-04-22 10:35:35,804:INFO:Uploading model into container now
2024-04-22 10:35:35,804:INFO:_master_model_container: 9
2024-04-22 10:35:35,804:INFO:_display_container: 2
2024-04-22 10:35:35,804:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=69)
2024-04-22 10:35:35,804:INFO:create_model() successfully completed......................................
2024-04-22 10:35:35,958:INFO:SubProcess create_model() end ==================================
2024-04-22 10:35:35,958:INFO:Creating metrics dataframe
2024-04-22 10:35:35,990:INFO:Initializing Gradient Boosting Classifier
2024-04-22 10:35:35,990:INFO:Total runtime is 0.6300002217292786 minutes
2024-04-22 10:35:35,993:INFO:SubProcess create_model() called ==================================
2024-04-22 10:35:35,993:INFO:Initializing create_model()
2024-04-22 10:35:35,993:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017733951C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:35:35,993:INFO:Checking exceptions
2024-04-22 10:35:35,993:INFO:Importing libraries
2024-04-22 10:35:35,993:INFO:Copying training dataset
2024-04-22 10:35:36,001:INFO:Defining folds
2024-04-22 10:35:36,001:INFO:Declaring metric variables
2024-04-22 10:35:36,007:INFO:Importing untrained model
2024-04-22 10:35:36,010:INFO:Gradient Boosting Classifier Imported successfully
2024-04-22 10:35:36,032:INFO:Starting cross validation
2024-04-22 10:35:36,036:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:35:36,590:INFO:Calculating mean and std
2024-04-22 10:35:36,590:INFO:Creating metrics dataframe
2024-04-22 10:35:36,590:INFO:Uploading results into container
2024-04-22 10:35:36,590:INFO:Uploading model into container now
2024-04-22 10:35:36,590:INFO:_master_model_container: 10
2024-04-22 10:35:36,590:INFO:_display_container: 2
2024-04-22 10:35:36,590:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=69, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-04-22 10:35:36,590:INFO:create_model() successfully completed......................................
2024-04-22 10:35:36,741:INFO:SubProcess create_model() end ==================================
2024-04-22 10:35:36,741:INFO:Creating metrics dataframe
2024-04-22 10:35:36,757:INFO:Initializing Linear Discriminant Analysis
2024-04-22 10:35:36,757:INFO:Total runtime is 0.642782469590505 minutes
2024-04-22 10:35:36,760:INFO:SubProcess create_model() called ==================================
2024-04-22 10:35:36,760:INFO:Initializing create_model()
2024-04-22 10:35:36,760:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017733951C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:35:36,760:INFO:Checking exceptions
2024-04-22 10:35:36,760:INFO:Importing libraries
2024-04-22 10:35:36,760:INFO:Copying training dataset
2024-04-22 10:35:36,770:INFO:Defining folds
2024-04-22 10:35:36,770:INFO:Declaring metric variables
2024-04-22 10:35:36,776:INFO:Importing untrained model
2024-04-22 10:35:36,787:INFO:Linear Discriminant Analysis Imported successfully
2024-04-22 10:35:36,801:INFO:Starting cross validation
2024-04-22 10:35:36,802:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:35:36,994:INFO:Calculating mean and std
2024-04-22 10:35:36,994:INFO:Creating metrics dataframe
2024-04-22 10:35:37,001:INFO:Uploading results into container
2024-04-22 10:35:37,001:INFO:Uploading model into container now
2024-04-22 10:35:37,001:INFO:_master_model_container: 11
2024-04-22 10:35:37,001:INFO:_display_container: 2
2024-04-22 10:35:37,001:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-04-22 10:35:37,004:INFO:create_model() successfully completed......................................
2024-04-22 10:35:37,170:INFO:SubProcess create_model() end ==================================
2024-04-22 10:35:37,170:INFO:Creating metrics dataframe
2024-04-22 10:35:37,180:INFO:Initializing Extra Trees Classifier
2024-04-22 10:35:37,180:INFO:Total runtime is 0.6498295744260153 minutes
2024-04-22 10:35:37,191:INFO:SubProcess create_model() called ==================================
2024-04-22 10:35:37,191:INFO:Initializing create_model()
2024-04-22 10:35:37,191:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017733951C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:35:37,191:INFO:Checking exceptions
2024-04-22 10:35:37,191:INFO:Importing libraries
2024-04-22 10:35:37,191:INFO:Copying training dataset
2024-04-22 10:35:37,200:INFO:Defining folds
2024-04-22 10:35:37,200:INFO:Declaring metric variables
2024-04-22 10:35:37,206:INFO:Importing untrained model
2024-04-22 10:35:37,219:INFO:Extra Trees Classifier Imported successfully
2024-04-22 10:35:37,240:INFO:Starting cross validation
2024-04-22 10:35:37,242:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:35:38,912:INFO:Calculating mean and std
2024-04-22 10:35:38,914:INFO:Creating metrics dataframe
2024-04-22 10:35:38,920:INFO:Uploading results into container
2024-04-22 10:35:38,920:INFO:Uploading model into container now
2024-04-22 10:35:38,923:INFO:_master_model_container: 12
2024-04-22 10:35:38,923:INFO:_display_container: 2
2024-04-22 10:35:38,923:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=69, verbose=0, warm_start=False)
2024-04-22 10:35:38,923:INFO:create_model() successfully completed......................................
2024-04-22 10:35:39,117:INFO:SubProcess create_model() end ==================================
2024-04-22 10:35:39,117:INFO:Creating metrics dataframe
2024-04-22 10:35:39,133:INFO:Initializing Light Gradient Boosting Machine
2024-04-22 10:35:39,133:INFO:Total runtime is 0.6823751449584962 minutes
2024-04-22 10:35:39,133:INFO:SubProcess create_model() called ==================================
2024-04-22 10:35:39,133:INFO:Initializing create_model()
2024-04-22 10:35:39,133:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017733951C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:35:39,140:INFO:Checking exceptions
2024-04-22 10:35:39,140:INFO:Importing libraries
2024-04-22 10:35:39,140:INFO:Copying training dataset
2024-04-22 10:35:39,140:INFO:Defining folds
2024-04-22 10:35:39,140:INFO:Declaring metric variables
2024-04-22 10:35:39,152:INFO:Importing untrained model
2024-04-22 10:35:39,152:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:35:39,170:INFO:Starting cross validation
2024-04-22 10:35:39,170:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:35:39,656:INFO:Calculating mean and std
2024-04-22 10:35:39,656:INFO:Creating metrics dataframe
2024-04-22 10:35:39,662:INFO:Uploading results into container
2024-04-22 10:35:39,662:INFO:Uploading model into container now
2024-04-22 10:35:39,665:INFO:_master_model_container: 13
2024-04-22 10:35:39,665:INFO:_display_container: 2
2024-04-22 10:35:39,665:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:35:39,665:INFO:create_model() successfully completed......................................
2024-04-22 10:35:39,836:INFO:SubProcess create_model() end ==================================
2024-04-22 10:35:39,836:INFO:Creating metrics dataframe
2024-04-22 10:35:39,855:INFO:Initializing CatBoost Classifier
2024-04-22 10:35:39,855:INFO:Total runtime is 0.6944233258565268 minutes
2024-04-22 10:35:39,857:INFO:SubProcess create_model() called ==================================
2024-04-22 10:35:39,860:INFO:Initializing create_model()
2024-04-22 10:35:39,860:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=catboost, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017733951C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:35:39,860:INFO:Checking exceptions
2024-04-22 10:35:39,860:INFO:Importing libraries
2024-04-22 10:35:39,860:INFO:Copying training dataset
2024-04-22 10:35:39,866:INFO:Defining folds
2024-04-22 10:35:39,866:INFO:Declaring metric variables
2024-04-22 10:35:39,873:INFO:Importing untrained model
2024-04-22 10:35:39,891:INFO:CatBoost Classifier Imported successfully
2024-04-22 10:35:39,910:INFO:Starting cross validation
2024-04-22 10:35:39,910:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:35:51,105:INFO:Calculating mean and std
2024-04-22 10:35:51,130:INFO:Creating metrics dataframe
2024-04-22 10:35:51,147:INFO:Uploading results into container
2024-04-22 10:35:51,147:INFO:Uploading model into container now
2024-04-22 10:35:51,150:INFO:_master_model_container: 14
2024-04-22 10:35:51,150:INFO:_display_container: 2
2024-04-22 10:35:51,150:INFO:<catboost.core.CatBoostClassifier object at 0x0000017732A6FC50>
2024-04-22 10:35:51,150:INFO:create_model() successfully completed......................................
2024-04-22 10:35:51,553:INFO:SubProcess create_model() end ==================================
2024-04-22 10:35:51,553:INFO:Creating metrics dataframe
2024-04-22 10:35:51,590:INFO:Initializing Dummy Classifier
2024-04-22 10:35:51,590:INFO:Total runtime is 0.8899956663449606 minutes
2024-04-22 10:35:51,595:INFO:SubProcess create_model() called ==================================
2024-04-22 10:35:51,596:INFO:Initializing create_model()
2024-04-22 10:35:51,596:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000017733951C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:35:51,596:INFO:Checking exceptions
2024-04-22 10:35:51,597:INFO:Importing libraries
2024-04-22 10:35:51,597:INFO:Copying training dataset
2024-04-22 10:35:51,605:INFO:Defining folds
2024-04-22 10:35:51,605:INFO:Declaring metric variables
2024-04-22 10:35:51,611:INFO:Importing untrained model
2024-04-22 10:35:51,616:INFO:Dummy Classifier Imported successfully
2024-04-22 10:35:51,643:INFO:Starting cross validation
2024-04-22 10:35:51,645:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:35:51,830:INFO:Calculating mean and std
2024-04-22 10:35:51,830:INFO:Creating metrics dataframe
2024-04-22 10:35:51,838:INFO:Uploading results into container
2024-04-22 10:35:51,840:INFO:Uploading model into container now
2024-04-22 10:35:51,840:INFO:_master_model_container: 15
2024-04-22 10:35:51,840:INFO:_display_container: 2
2024-04-22 10:35:51,840:INFO:DummyClassifier(constant=None, random_state=69, strategy='prior')
2024-04-22 10:35:51,840:INFO:create_model() successfully completed......................................
2024-04-22 10:35:52,021:INFO:SubProcess create_model() end ==================================
2024-04-22 10:35:52,021:INFO:Creating metrics dataframe
2024-04-22 10:35:52,115:INFO:Initializing create_model()
2024-04-22 10:35:52,115:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:35:52,115:INFO:Checking exceptions
2024-04-22 10:35:52,125:INFO:Importing libraries
2024-04-22 10:35:52,125:INFO:Copying training dataset
2024-04-22 10:35:52,131:INFO:Defining folds
2024-04-22 10:35:52,131:INFO:Declaring metric variables
2024-04-22 10:35:52,131:INFO:Importing untrained model
2024-04-22 10:35:52,131:INFO:Declaring custom model
2024-04-22 10:35:52,135:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:35:52,136:INFO:Cross validation set to False
2024-04-22 10:35:52,136:INFO:Fitting Model
2024-04-22 10:35:53,452:INFO:[LightGBM] [Info] Number of positive: 54, number of negative: 46
2024-04-22 10:35:53,465:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000909 seconds.
2024-04-22 10:35:53,465:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-04-22 10:35:53,465:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-04-22 10:35:53,467:INFO:[LightGBM] [Info] Total Bins 31
2024-04-22 10:35:53,470:INFO:[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 6
2024-04-22 10:35:53,473:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.540000 -> initscore=0.160343
2024-04-22 10:35:53,473:INFO:[LightGBM] [Info] Start training from score 0.160343
2024-04-22 10:35:53,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:35:53,767:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:35:53,767:INFO:create_model() successfully completed......................................
2024-04-22 10:35:54,061:INFO:_master_model_container: 15
2024-04-22 10:35:54,062:INFO:_display_container: 2
2024-04-22 10:35:54,062:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:35:54,062:INFO:compare_models() successfully completed......................................
2024-04-22 10:37:16,612:INFO:Initializing create_model()
2024-04-22 10:37:16,621:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:37:16,621:INFO:Checking exceptions
2024-04-22 10:37:17,020:INFO:Importing libraries
2024-04-22 10:37:17,025:INFO:Copying training dataset
2024-04-22 10:37:17,061:INFO:Defining folds
2024-04-22 10:37:17,061:INFO:Declaring metric variables
2024-04-22 10:37:17,070:INFO:Importing untrained model
2024-04-22 10:37:17,084:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:37:17,098:INFO:Starting cross validation
2024-04-22 10:37:17,130:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:37:18,999:INFO:Calculating mean and std
2024-04-22 10:37:19,006:INFO:Creating metrics dataframe
2024-04-22 10:37:19,048:INFO:Finalizing model
2024-04-22 10:37:19,168:INFO:[LightGBM] [Info] Number of positive: 54, number of negative: 46
2024-04-22 10:37:19,170:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000125 seconds.
2024-04-22 10:37:19,170:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-04-22 10:37:19,170:INFO:[LightGBM] [Info] Total Bins 31
2024-04-22 10:37:19,170:INFO:[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 6
2024-04-22 10:37:19,176:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.540000 -> initscore=0.160343
2024-04-22 10:37:19,176:INFO:[LightGBM] [Info] Start training from score 0.160343
2024-04-22 10:37:19,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:19,235:INFO:Uploading results into container
2024-04-22 10:37:19,240:INFO:Uploading model into container now
2024-04-22 10:37:19,300:INFO:_master_model_container: 16
2024-04-22 10:37:19,300:INFO:_display_container: 3
2024-04-22 10:37:19,304:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:37:19,304:INFO:create_model() successfully completed......................................
2024-04-22 10:37:34,291:INFO:Initializing create_model()
2024-04-22 10:37:34,293:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:37:34,293:INFO:Checking exceptions
2024-04-22 10:37:34,374:INFO:Importing libraries
2024-04-22 10:37:34,380:INFO:Copying training dataset
2024-04-22 10:37:34,391:INFO:Defining folds
2024-04-22 10:37:34,391:INFO:Declaring metric variables
2024-04-22 10:37:34,398:INFO:Importing untrained model
2024-04-22 10:37:34,400:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:37:34,412:INFO:Starting cross validation
2024-04-22 10:37:34,415:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:37:37,123:INFO:Calculating mean and std
2024-04-22 10:37:37,124:INFO:Creating metrics dataframe
2024-04-22 10:37:37,147:INFO:Finalizing model
2024-04-22 10:37:37,202:INFO:[LightGBM] [Info] Number of positive: 54, number of negative: 46
2024-04-22 10:37:37,202:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000051 seconds.
2024-04-22 10:37:37,202:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-04-22 10:37:37,202:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-04-22 10:37:37,202:INFO:[LightGBM] [Info] Total Bins 31
2024-04-22 10:37:37,202:INFO:[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 6
2024-04-22 10:37:37,209:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.540000 -> initscore=0.160343
2024-04-22 10:37:37,209:INFO:[LightGBM] [Info] Start training from score 0.160343
2024-04-22 10:37:37,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:37:37,267:INFO:Uploading results into container
2024-04-22 10:37:37,269:INFO:Uploading model into container now
2024-04-22 10:37:37,313:INFO:_master_model_container: 17
2024-04-22 10:37:37,313:INFO:_display_container: 4
2024-04-22 10:37:37,316:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:37:37,316:INFO:create_model() successfully completed......................................
2024-04-22 10:37:39,020:INFO:Initializing evaluate_model()
2024-04-22 10:37:39,020:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-04-22 10:37:39,153:INFO:Initializing plot_model()
2024-04-22 10:37:39,153:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-04-22 10:37:39,155:INFO:Checking exceptions
2024-04-22 10:37:39,164:INFO:Preloading libraries
2024-04-22 10:37:39,276:INFO:Copying training dataset
2024-04-22 10:37:39,276:INFO:Plot type: pipeline
2024-04-22 10:37:40,308:INFO:Visual Rendered Successfully
2024-04-22 10:37:40,507:INFO:plot_model() successfully completed......................................
2024-04-22 10:38:19,549:INFO:Initializing create_model()
2024-04-22 10:38:19,560:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:38:19,560:INFO:Checking exceptions
2024-04-22 10:38:22,075:INFO:Importing libraries
2024-04-22 10:38:22,190:INFO:Copying training dataset
2024-04-22 10:38:22,506:INFO:Defining folds
2024-04-22 10:38:22,507:INFO:Declaring metric variables
2024-04-22 10:38:22,525:INFO:Importing untrained model
2024-04-22 10:38:22,607:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:38:22,632:INFO:Starting cross validation
2024-04-22 10:38:22,785:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:38:31,931:INFO:Calculating mean and std
2024-04-22 10:38:31,952:INFO:Creating metrics dataframe
2024-04-22 10:38:32,023:INFO:Finalizing model
2024-04-22 10:38:32,203:INFO:[LightGBM] [Info] Number of positive: 54, number of negative: 46
2024-04-22 10:38:32,208:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000300 seconds.
2024-04-22 10:38:32,208:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-04-22 10:38:32,210:INFO:[LightGBM] [Info] Total Bins 31
2024-04-22 10:38:32,215:INFO:[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 6
2024-04-22 10:38:32,218:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.540000 -> initscore=0.160343
2024-04-22 10:38:32,218:INFO:[LightGBM] [Info] Start training from score 0.160343
2024-04-22 10:38:32,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:38:32,285:INFO:Uploading results into container
2024-04-22 10:38:32,290:INFO:Uploading model into container now
2024-04-22 10:38:32,366:INFO:_master_model_container: 18
2024-04-22 10:38:32,366:INFO:_display_container: 5
2024-04-22 10:38:32,380:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:38:32,390:INFO:create_model() successfully completed......................................
2024-04-22 10:38:39,150:INFO:Initializing tune_model()
2024-04-22 10:38:39,173:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-04-22 10:38:39,173:INFO:Checking exceptions
2024-04-22 10:38:39,302:INFO:Copying training dataset
2024-04-22 10:38:39,307:INFO:Checking base model
2024-04-22 10:38:39,307:INFO:Base model : Light Gradient Boosting Machine
2024-04-22 10:38:39,316:INFO:Declaring metric variables
2024-04-22 10:38:39,320:INFO:Defining Hyperparameters
2024-04-22 10:38:39,542:INFO:Tuning with n_jobs=-1
2024-04-22 10:38:39,542:INFO:Initializing RandomizedSearchCV
2024-04-22 10:42:32,579:INFO:Initializing create_model()
2024-04-22 10:42:32,587:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:42:32,587:INFO:Checking exceptions
2024-04-22 10:42:33,063:INFO:Importing libraries
2024-04-22 10:42:33,065:INFO:Copying training dataset
2024-04-22 10:42:33,115:INFO:Defining folds
2024-04-22 10:42:33,120:INFO:Declaring metric variables
2024-04-22 10:42:33,127:INFO:Importing untrained model
2024-04-22 10:42:33,153:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:42:33,175:INFO:Starting cross validation
2024-04-22 10:42:33,224:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:42:52,995:INFO:Calculating mean and std
2024-04-22 10:42:53,052:INFO:Creating metrics dataframe
2024-04-22 10:42:53,200:INFO:Finalizing model
2024-04-22 10:42:53,569:INFO:[LightGBM] [Info] Number of positive: 54, number of negative: 46
2024-04-22 10:42:53,572:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000828 seconds.
2024-04-22 10:42:53,572:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-04-22 10:42:53,574:INFO:[LightGBM] [Info] Total Bins 31
2024-04-22 10:42:53,580:INFO:[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 6
2024-04-22 10:42:53,586:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.540000 -> initscore=0.160343
2024-04-22 10:42:53,586:INFO:[LightGBM] [Info] Start training from score 0.160343
2024-04-22 10:42:53,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:42:53,708:INFO:Uploading results into container
2024-04-22 10:42:53,711:INFO:Uploading model into container now
2024-04-22 10:42:53,830:INFO:_master_model_container: 19
2024-04-22 10:42:53,830:INFO:_display_container: 6
2024-04-22 10:42:53,847:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:42:53,848:INFO:create_model() successfully completed......................................
2024-04-22 10:42:59,744:INFO:Initializing tune_model()
2024-04-22 10:42:59,744:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2024-04-22 10:42:59,744:INFO:Checking exceptions
2024-04-22 10:42:59,799:INFO:Copying training dataset
2024-04-22 10:42:59,799:INFO:Checking base model
2024-04-22 10:42:59,799:INFO:Base model : Light Gradient Boosting Machine
2024-04-22 10:42:59,808:INFO:Declaring metric variables
2024-04-22 10:42:59,812:INFO:Defining Hyperparameters
2024-04-22 10:43:00,087:INFO:Tuning with n_jobs=-1
2024-04-22 10:43:00,087:INFO:Initializing RandomizedSearchCV
2024-04-22 10:43:47,870:INFO:best_params: {'actual_estimator__reg_lambda': 0.001, 'actual_estimator__reg_alpha': 1, 'actual_estimator__num_leaves': 10, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.7, 'actual_estimator__min_child_samples': 16, 'actual_estimator__learning_rate': 0.05, 'actual_estimator__feature_fraction': 0.7, 'actual_estimator__bagging_freq': 6, 'actual_estimator__bagging_fraction': 0.6}
2024-04-22 10:43:47,920:INFO:Hyperparameter search completed
2024-04-22 10:43:47,922:INFO:SubProcess create_model() called ==================================
2024-04-22 10:43:47,930:INFO:Initializing create_model()
2024-04-22 10:43:47,930:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001772E763E10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 0.001, 'reg_alpha': 1, 'num_leaves': 10, 'n_estimators': 250, 'min_split_gain': 0.7, 'min_child_samples': 16, 'learning_rate': 0.05, 'feature_fraction': 0.7, 'bagging_freq': 6, 'bagging_fraction': 0.6})
2024-04-22 10:43:47,930:INFO:Checking exceptions
2024-04-22 10:43:47,930:INFO:Importing libraries
2024-04-22 10:43:47,940:INFO:Copying training dataset
2024-04-22 10:43:47,990:INFO:Defining folds
2024-04-22 10:43:47,995:INFO:Declaring metric variables
2024-04-22 10:43:48,030:INFO:Importing untrained model
2024-04-22 10:43:48,030:INFO:Declaring custom model
2024-04-22 10:43:48,040:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:43:48,050:INFO:Starting cross validation
2024-04-22 10:43:48,060:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:43:56,033:INFO:Calculating mean and std
2024-04-22 10:43:56,035:INFO:Creating metrics dataframe
2024-04-22 10:43:56,060:INFO:Finalizing model
2024-04-22 10:43:56,120:INFO:[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7
2024-04-22 10:43:56,120:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-04-22 10:43:56,120:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-04-22 10:43:56,130:INFO:[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7
2024-04-22 10:43:56,130:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-04-22 10:43:56,130:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-04-22 10:43:56,130:INFO:[LightGBM] [Info] Number of positive: 54, number of negative: 46
2024-04-22 10:43:56,132:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.
2024-04-22 10:43:56,132:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-04-22 10:43:56,132:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-04-22 10:43:56,132:INFO:[LightGBM] [Info] Total Bins 31
2024-04-22 10:43:56,132:INFO:[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 6
2024-04-22 10:43:56,137:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.540000 -> initscore=0.160343
2024-04-22 10:43:56,137:INFO:[LightGBM] [Info] Start training from score 0.160343
2024-04-22 10:43:56,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,150:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,162:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,165:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,165:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,175:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,179:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,179:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,179:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,179:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,179:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,179:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,179:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,180:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,182:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,182:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:56,183:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:43:56,210:INFO:Uploading results into container
2024-04-22 10:43:56,210:INFO:Uploading model into container now
2024-04-22 10:43:56,220:INFO:_master_model_container: 20
2024-04-22 10:43:56,220:INFO:_display_container: 7
2024-04-22 10:43:56,220:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:43:56,220:INFO:create_model() successfully completed......................................
2024-04-22 10:43:58,338:INFO:SubProcess create_model() end ==================================
2024-04-22 10:43:58,340:INFO:choose_better activated
2024-04-22 10:43:58,354:INFO:SubProcess create_model() called ==================================
2024-04-22 10:43:58,360:INFO:Initializing create_model()
2024-04-22 10:43:58,360:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:43:58,360:INFO:Checking exceptions
2024-04-22 10:43:58,366:INFO:Importing libraries
2024-04-22 10:43:58,370:INFO:Copying training dataset
2024-04-22 10:43:58,370:INFO:Defining folds
2024-04-22 10:43:58,370:INFO:Declaring metric variables
2024-04-22 10:43:58,370:INFO:Importing untrained model
2024-04-22 10:43:58,370:INFO:Declaring custom model
2024-04-22 10:43:58,370:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:43:58,370:INFO:Starting cross validation
2024-04-22 10:43:58,382:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-04-22 10:43:59,540:INFO:Calculating mean and std
2024-04-22 10:43:59,542:INFO:Creating metrics dataframe
2024-04-22 10:43:59,542:INFO:Finalizing model
2024-04-22 10:43:59,560:INFO:[LightGBM] [Info] Number of positive: 54, number of negative: 46
2024-04-22 10:43:59,560:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000196 seconds.
2024-04-22 10:43:59,560:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-04-22 10:43:59,560:INFO:[LightGBM] [Info] Total Bins 31
2024-04-22 10:43:59,560:INFO:[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 6
2024-04-22 10:43:59,560:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.540000 -> initscore=0.160343
2024-04-22 10:43:59,560:INFO:[LightGBM] [Info] Start training from score 0.160343
2024-04-22 10:43:59,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:43:59,632:INFO:Uploading results into container
2024-04-22 10:43:59,633:INFO:Uploading model into container now
2024-04-22 10:43:59,633:INFO:_master_model_container: 21
2024-04-22 10:43:59,633:INFO:_display_container: 8
2024-04-22 10:43:59,633:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:43:59,633:INFO:create_model() successfully completed......................................
2024-04-22 10:43:59,831:INFO:SubProcess create_model() end ==================================
2024-04-22 10:43:59,832:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=69, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.66
2024-04-22 10:43:59,835:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.68
2024-04-22 10:43:59,835:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2024-04-22 10:43:59,835:INFO:choose_better completed
2024-04-22 10:43:59,850:INFO:_master_model_container: 21
2024-04-22 10:43:59,850:INFO:_display_container: 7
2024-04-22 10:43:59,850:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:43:59,850:INFO:tune_model() successfully completed......................................
2024-04-22 10:44:00,040:INFO:Initializing evaluate_model()
2024-04-22 10:44:00,040:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2024-04-22 10:44:00,052:INFO:Initializing plot_model()
2024-04-22 10:44:00,052:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2024-04-22 10:44:00,052:INFO:Checking exceptions
2024-04-22 10:44:00,058:INFO:Preloading libraries
2024-04-22 10:44:00,061:INFO:Copying training dataset
2024-04-22 10:44:00,061:INFO:Plot type: pipeline
2024-04-22 10:44:00,490:INFO:Visual Rendered Successfully
2024-04-22 10:44:00,710:INFO:plot_model() successfully completed......................................
2024-04-22 10:44:00,715:INFO:Initializing finalize_model()
2024-04-22 10:44:00,715:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-04-22 10:44:00,715:INFO:Finalizing LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-04-22 10:44:00,722:INFO:Initializing create_model()
2024-04-22 10:44:00,722:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001772E773410>, estimator=LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-04-22 10:44:00,722:INFO:Checking exceptions
2024-04-22 10:44:00,725:INFO:Importing libraries
2024-04-22 10:44:00,725:INFO:Copying training dataset
2024-04-22 10:44:00,725:INFO:Defining folds
2024-04-22 10:44:00,725:INFO:Declaring metric variables
2024-04-22 10:44:00,725:INFO:Importing untrained model
2024-04-22 10:44:00,725:INFO:Declaring custom model
2024-04-22 10:44:00,726:INFO:Light Gradient Boosting Machine Imported successfully
2024-04-22 10:44:00,726:INFO:Cross validation set to False
2024-04-22 10:44:00,726:INFO:Fitting Model
2024-04-22 10:44:00,742:INFO:[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7
2024-04-22 10:44:00,742:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-04-22 10:44:00,742:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-04-22 10:44:00,746:INFO:[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7
2024-04-22 10:44:00,746:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2024-04-22 10:44:00,746:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2024-04-22 10:44:00,746:INFO:[LightGBM] [Info] Number of positive: 77, number of negative: 66
2024-04-22 10:44:00,746:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000076 seconds.
2024-04-22 10:44:00,746:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-04-22 10:44:00,746:INFO:[LightGBM] [Info] Total Bins 33
2024-04-22 10:44:00,746:INFO:[LightGBM] [Info] Number of data points in the train set: 143, number of used features: 6
2024-04-22 10:44:00,746:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.538462 -> initscore=0.154151
2024-04-22 10:44:00,746:INFO:[LightGBM] [Info] Start training from score 0.154151
2024-04-22 10:44:00,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,760:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,760:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,760:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,760:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,760:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,760:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,764:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,764:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,764:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,777:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,780:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,780:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,780:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,780:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,780:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,780:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,781:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,781:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,781:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,798:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,800:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,800:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,800:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,800:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,800:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,800:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2024-04-22 10:44:00,800:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2024-04-22 10:44:00,810:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'vents'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose='deprecated'))),
                ('cate...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.7,
                                importance_type='split', learning_rate=0.05,
                                max_depth=-1, min_child_samples=16,
                                min_child_weight=0.001, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                objective=None, random_state=69, reg_alpha=1,
                                reg_lambda=0.001, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-04-22 10:44:00,810:INFO:create_model() successfully completed......................................
2024-04-22 10:44:00,998:INFO:_master_model_container: 21
2024-04-22 10:44:00,998:INFO:_display_container: 7
2024-04-22 10:44:01,005:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'vents'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose='deprecated'))),
                ('cate...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.7,
                                importance_type='split', learning_rate=0.05,
                                max_depth=-1, min_child_samples=16,
                                min_child_weight=0.001, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                objective=None, random_state=69, reg_alpha=1,
                                reg_lambda=0.001, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-04-22 10:44:01,005:INFO:finalize_model() successfully completed......................................
2024-04-22 10:44:01,204:INFO:Initializing save_model()
2024-04-22 10:44:01,204:INFO:save_model(model=LGBMClassifier(bagging_fraction=0.6, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=250, n_jobs=-1, num_leaves=10, objective=None,
               random_state=69, reg_alpha=1, reg_lambda=0.001, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=happiness_pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\BAGAS\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'vents'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strate...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-04-22 10:44:01,204:INFO:Adding model into prep_pipe
2024-04-22 10:44:01,220:INFO:happiness_pipeline.pkl saved in current working directory
2024-04-22 10:44:01,228:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['infoavail', 'housecost',
                                             'schoolquality', 'policetrust',
                                             'streetquality', 'vents'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean',
                                                              verbose='deprecated'))),
                ('cate...
                                boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, feature_fraction=0.7,
                                importance_type='split', learning_rate=0.05,
                                max_depth=-1, min_child_samples=16,
                                min_child_weight=0.001, min_split_gain=0.7,
                                n_estimators=250, n_jobs=-1, num_leaves=10,
                                objective=None, random_state=69, reg_alpha=1,
                                reg_lambda=0.001, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2024-04-22 10:44:01,228:INFO:save_model() successfully completed......................................
